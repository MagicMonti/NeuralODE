{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "#from torch import Tensor as tensor\n",
    "from torch import nn\n",
    "from torch.nn  import functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from scipy.integrate import odeint\n",
    "from scipy.integrate import RK45\n",
    "\n",
    "\n",
    "from torch.autograd.functional import jacobian\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available() #also worsk for AMD-ROCm\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pendelum\n",
    "$$\\frac{d^2 \\theta}{dt^2}  +  \\lambda\\frac{d\\theta}{dt} +  \\frac{g}{l}sin(\\theta) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pend(y,t,b,c):\n",
    "    theta, omega = y\n",
    "    dydt = [omega, -b*omega - c*np.sin(theta)]\n",
    "    return dydt\n",
    "\n",
    "b = 0.25\n",
    "c = 5\n",
    "\n",
    "theta_0 = np.pi\n",
    "omega_0 = 1\n",
    " \n",
    "y0 = [theta_0 , omega_0]\n",
    "\n",
    "t_final = 10\n",
    "\n",
    "t = np.arange(0, 10, 0.01)\n",
    "\n",
    "sol = odeint(pend,y0, t, args=(b,c))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = 2*np.pi - sol[:, 0]\n",
    "omega = sol[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(t, theta, 'b', label='theta(t)')\n",
    "plt.plot(t, omega, 'g', label='omega(t)')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('t')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural ODE with ajoint-sensitivity-method\n",
    "\n",
    "$$\n",
    "x: \\mathbb{R} \\rightarrow  \\mathbb{R}^{N_1 \\times N_2} \\\\\n",
    "N_1 \\ldots \\text{size of batch} \\\\\n",
    "N_2 \\ldots \\text{size of vector} \\\\\n",
    "x^i_j \\equiv x(t) \\\\\n",
    "$$\n",
    "$$\n",
    "\\mathcal{L} : \\mathbb{R}^{N_1 \\times N_2} \\times \\underbrace{\\mathbb{R}^{N_1 \\times N_2}}_{\\text{prediction}}  \\rightarrow \\mathbb{R} \\\\\n",
    "$$\n",
    "$$\n",
    "a^i_j \\equiv a = \\frac{\\partial \\mathcal{L}}{\\partial x(t)} \\\\\n",
    "\\dot x = f(x(t), p^a) \\\\\n",
    "\n",
    "\\frac{da}{dt} = - a^T \\frac{\\partial f}{\\partial x} \\\\\n",
    "\n",
    "\\frac{d\\mathcal{L}}{dp^a} = - \\int_{t_1}^{t_0} dt \\sum_i \\sum_j a^j_i \\frac{\\partial f^i_j}{\\partial p^a } = - N_1 \\ N_2 \\int dt \\ \\frac{\\partial}{\\partial p^a}   mean(a(t) * f(x(t)))\n",
    "$$\n",
    "\n",
    "$$\n",
    "= - N_1 \\ N_2 * \\sum_n \\frac{\\partial}{\\partial p^a} mean(a_n * f(x_n))\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_n = \\text{from ode solve last x at last point} \\\\\n",
    "\\text{loop : } \\\\ \n",
    "x_{n-1} = x_n - \\Delta t * f(x_n, p)\n",
    "$$\n",
    "\n",
    "$$\n",
    "a_n = \\text{at last point} \\\\\n",
    "\\text{loop : } \\\\ \n",
    "a_{n-1} = a_n - \\Delta t * f(x_n, p)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ode_solve(func, x, propagation_time, delta_t, return_seq=False):\n",
    "    \"\"\"\n",
    "    solves the differential equation : dx/dt = f(x(t)) using eulers method\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    func :  python function\n",
    "            Parameters\n",
    "            ----------\n",
    "            x : torch.Tensor    \n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            torch.Tensor\n",
    "    \n",
    "    x : torch.Tensor\n",
    "        input for func (singlet or whole batch)\n",
    "\n",
    "    propagation_time : float\n",
    "    delta_t : float \n",
    "        accuracy of solution\n",
    "    return_seq : bool\n",
    "        flag for returning whole sequence or not\n",
    "\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    torch.tensor\n",
    "\n",
    "    \"\"\"\n",
    "    time = torch.arange(start=0, end=propagation_time, step=delta_t, requires_grad=False)\n",
    "    x = x.clone().detach()\n",
    "    if not return_seq:\n",
    "        for t in time:\n",
    "            x = x + delta_t * func(x)\n",
    "        return x\n",
    "    if return_seq:\n",
    "        outputs = torch.zeros(time.shape[0], x.shape[1])\n",
    "        for i,_ in enumerate(time):\n",
    "            x = x + delta_t * func(x)\n",
    "            outputs[i] = x\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PendelumNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PendelumNetwork, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(2, 10, dtype=float),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(10, 2, dtype=float)\n",
    "        )\n",
    "    def forward(self,x): \n",
    "        \"\"\"implements f(x)\n",
    "        x = (theta, omega)\n",
    "        theta ... angle\n",
    "        omega ... angular velocity\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.tensor \n",
    "            shape = (2,N)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.tensor\n",
    "\n",
    "        \"\"\"\n",
    "        return self.linear_relu_stack(x)\n",
    "\n",
    "    def flatten_parameters(self):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor: \n",
    "            flat parameters\n",
    "        \"\"\"\n",
    "        flat_parameters = []\n",
    "        for p in self.parameters():\n",
    "            flat_parameters.append(p.flatten())\n",
    "        return torch.cat(flat_parameters)\n",
    "\n",
    "    def get_adfdp(self, x , a ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "        a : torch.Tensor\n",
    "            same shape as x\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor : \n",
    "            a * df/dp ... p = Network parameters\n",
    "        \"\"\"\n",
    "        with torch.enable_grad():\n",
    "\n",
    "            temp = torch.mean( a.detach().T @ self.forward(x.requires_grad_(True)))\n",
    "            temp.backward(retain_graph=True)\n",
    "            grads = []\n",
    "            for param in self.parameters():\n",
    "                grads.append(param.grad.view(-1))\n",
    "            grads = torch.cat(grads)\n",
    "\n",
    "            for param in self.parameters():\n",
    "                param.grad.zero_() #clears accumulation\n",
    "            return grads\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODEAdjoint(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, flat_parameters, func, delta_t, propagation_time, return_seq=False):\n",
    "        \n",
    "        if return_seq:\n",
    "            return ode_solve(func, x,  propagation_time, delta_t, return_seq)\n",
    "        else:\n",
    "            output = ode_solve(func, x, propagation_time, delta_t, return_seq)\n",
    "            ctx.func = func\n",
    "            ctx.delta_t = delta_t\n",
    "            ctx.propagation_time = propagation_time\n",
    "            ctx.save_for_backward(output.clone(), flat_parameters)\n",
    "\n",
    "\n",
    "            return output\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(ctx, dL_dz):\n",
    "\n",
    "        func = ctx.func\n",
    "        delta_t = ctx.delta_t\n",
    "        propagation_time = ctx.propagation_time\n",
    "        x_1 , flat_parameters = ctx.saved_tensors\n",
    "        a_1 =  (-1) * dL_dz.detach() \n",
    "\n",
    "        def one_adjoint_backward_step(func, x_1, a_1, delta_t):\n",
    "            a = a_1.clone().detach()\n",
    "            x = x_1.clone().detach().requires_grad_(True) #new graph drop old graph\n",
    "            with torch.enable_grad():\n",
    "                outputs = func(x)\n",
    "                a -= delta_t * torch.autograd.grad(outputs=outputs, inputs=x, grad_outputs=a)[0]\n",
    "                return a\n",
    "\n",
    "        def one_euler_backward_step(func,x_1, delta_t):\n",
    "            x = x_1.clone().detach()\n",
    "            x -= delta_t * func(x)\n",
    "            return x\n",
    "\n",
    "        def get_dL_dp(func, a_1, x_1, delta_t):\n",
    "            time = torch.arange(start=0, end=propagation_time, step=delta_t, requires_grad=False)\n",
    "            a = a_1.clone().detach() \n",
    "            x = x_1.clone().detach()\n",
    "            result = torch.zeros(func.flatten_parameters().shape)\n",
    "            for t in time:\n",
    "                result -= delta_t * func.get_adfdp(x,a)\n",
    "                a = one_adjoint_backward_step(func, x, a, delta_t).clone().detach()\n",
    "                x = one_euler_backward_step(func, x,delta_t).clone().detach()\n",
    "            return result\n",
    "\n",
    "        return None, get_dL_dp(func,a_1,x_1,delta_t), None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralODE(nn.Module):\n",
    "    def __init__(self, func, delta_t):\n",
    "        super(NeuralODE, self).__init__()\n",
    "        self.func = func\n",
    "        self.delta_t = delta_t\n",
    "    \n",
    "    def forward(self, x, propagation_time, return_seq=False, use_adjoint = True):\n",
    "        if use_adjoint:\n",
    "            return ODEAdjoint.apply(x, self.func.flatten_parameters(), self.func, self.delta_t , propagation_time, return_seq)\n",
    "        else:\n",
    "            return ode_solve(self.func, x,  propagation_time, self.delta_t, return_seq)\n",
    "           \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_set(theta, omega, propagation_index=25, number_of_samples=200):\n",
    "    \"\"\" x_i -> x_f\n",
    "        x_i ... initial state\n",
    "        x_f ... final state\n",
    "        x_i = [theta_i, omega_i]\n",
    "        x_f = [theta_f, omega_f]\n",
    "        data_set = [[x_i, x_f]\n",
    "                    [...]\n",
    "                    ...\n",
    "                    ]\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : np.array or torch.tensor\n",
    "        angle\n",
    "    omega : np.array or torch.tensor\n",
    "        angular velocity\n",
    "   \n",
    "    propagation_index : int\n",
    "    \n",
    "    number_of_samples : int\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    torch.tensor\n",
    "    \"\"\"\n",
    "    x_i = []\n",
    "    x_f = []\n",
    "    data_set = torch.zeros((2,number_of_samples,2), dtype=float)\n",
    "    while len(x_i) < number_of_samples:\n",
    "\n",
    "        index = int(random.uniform(0, len(theta))) #selectes random index\n",
    "\n",
    "        if index+propagation_index >= len(theta):\n",
    "            continue\n",
    "\n",
    "        x_i.append([theta[index], omega[index]])\n",
    "        x_f.append([theta[index+propagation_index], omega[index+propagation_index]])\n",
    "    x_i = torch.tensor(x_i)\n",
    "    x_f = torch.tensor(x_f)\n",
    "    data_set[0][:] = x_i\n",
    "    data_set[1][:] = x_f\n",
    "    return data_set.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(data_set, batch_size=64):\n",
    "    \"\"\"\n",
    "    creates batches\n",
    "    data is not shuffled and dropes last batch if size would be smaller than batch_size\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_set : troch.tensor \n",
    "        data_set[0] ... input\n",
    "        data_set[1] ... output\n",
    "    \n",
    "        shape = (2,N_1,N_2)\n",
    "        N_1 ... number of samples\n",
    "        N_2 ... size of vector\n",
    "\n",
    "    batch_size : int\n",
    "\n",
    "    Retruns\n",
    "    -------\n",
    "    troch.tensor\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    number_of_batches = int(data_set.shape[1]/batch_size)\n",
    "    batches = torch.zeros((number_of_batches,2,batch_size,data_set.shape[2]), dtype=float)\n",
    "    for i in range(number_of_batches-1):\n",
    "        batch = torch.zeros((2,batch_size,data_set.shape[2]), dtype=float)\n",
    "        batch[0] = data_set[0][i*batch_size:(i+1)*batch_size][:]\n",
    "        batch[1] = data_set[1][i*batch_size:(i+1)*batch_size][:] \n",
    "        batches[i] = batch\n",
    "    \n",
    "    return batches[:int(number_of_batches*0.75)], batches[int(number_of_batches*0.75):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3\n",
      "torch.Size([3, 2, 64, 2])\n",
      "torch.Size([2, 2, 64, 2])\n"
     ]
    }
   ],
   "source": [
    "propagation_index = 30\n",
    "propagation_time = t[propagation_index]\n",
    "print(propagation_time)\n",
    "data_set = create_data_set(theta=theta, omega=omega, propagation_index=25, number_of_samples=64*5)\n",
    "train, test = create_batches(data_set, 64)\n",
    "print(train.shape)\n",
    "print(test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.2148,  3.5213],\n",
      "        [-2.5903, -1.9096],\n",
      "        [-4.1069, -4.0488],\n",
      "        [ 1.9009,  1.2220],\n",
      "        [ 3.2402,  3.5108],\n",
      "        [-2.7330, -2.7784],\n",
      "        [ 1.2889,  0.7729],\n",
      "        [ 2.3473,  2.0933],\n",
      "        [-3.2645, -4.1737],\n",
      "        [ 2.3494,  1.4534],\n",
      "        [ 2.1319,  2.4574],\n",
      "        [-1.8065, -1.9580],\n",
      "        [-2.2930, -1.4527],\n",
      "        [-3.9463, -3.3063],\n",
      "        [-1.8839, -1.4437],\n",
      "        [ 1.7838,  0.7683],\n",
      "        [ 1.3936,  2.2417],\n",
      "        [-0.4413,  0.4476],\n",
      "        [ 3.2327,  2.6679],\n",
      "        [-0.3610, -1.2434],\n",
      "        [ 1.0015,  0.0992],\n",
      "        [ 2.3420,  2.0673],\n",
      "        [-4.1069, -4.0488],\n",
      "        [ 2.2160,  1.7370],\n",
      "        [-1.8176, -1.3001],\n",
      "        [ 1.7850,  1.0600],\n",
      "        [ 2.2142,  2.4361],\n",
      "        [ 0.4521,  1.2416],\n",
      "        [ 0.7281, -0.1791],\n",
      "        [-2.5675, -1.8704],\n",
      "        [ 1.7287,  1.6475],\n",
      "        [-1.9543, -1.7090],\n",
      "        [ 1.8754,  3.1327],\n",
      "        [-1.6174, -1.9634],\n",
      "        [-1.9347, -1.8499],\n",
      "        [ 1.5583,  2.3237],\n",
      "        [-3.5345, -2.7618],\n",
      "        [-1.0066, -0.1660],\n",
      "        [ 2.1988,  1.7034],\n",
      "        [ 1.7545,  1.0187],\n",
      "        [ 0.6734, -0.2601],\n",
      "        [ 0.4873, -0.5584],\n",
      "        [ 1.2889,  0.7729],\n",
      "        [ 1.0374,  0.3326],\n",
      "        [ 1.2188,  2.1414],\n",
      "        [ 2.1213,  1.5636],\n",
      "        [-0.0110, -1.2499],\n",
      "        [ 1.3912,  1.7266],\n",
      "        [ 1.0821,  2.0546],\n",
      "        [ 0.6878,  2.1737],\n",
      "        [ 1.2377,  0.6812],\n",
      "        [ 1.4108,  1.7283],\n",
      "        [ 3.4193,  3.2362],\n",
      "        [ 2.3357,  2.0406],\n",
      "        [ 2.4966,  3.4471],\n",
      "        [-0.4885, -1.3462],\n",
      "        [ 0.4397,  1.9264],\n",
      "        [-1.1583, -1.7938],\n",
      "        [ 1.5864,  1.6853],\n",
      "        [-1.1987, -0.3963],\n",
      "        [ 1.0739,  1.6284],\n",
      "        [-1.3287, -1.8753],\n",
      "        [-0.3610, -1.2434],\n",
      "        [ 1.6263,  1.4031]], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "10 \t training  8.481357534365001\n",
      " \t validation  nan\n",
      "_________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\z004m34v\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\z004m34v\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.1939,  3.5456],\n",
      "        [-2.3200, -1.7977],\n",
      "        [-3.8921, -3.9501],\n",
      "        [ 1.7273,  1.2119],\n",
      "        [ 3.2111,  3.5331],\n",
      "        [-2.5919, -2.7007],\n",
      "        [ 0.9307,  0.7085],\n",
      "        [ 2.2484,  2.1005],\n",
      "        [-3.2696, -4.1312],\n",
      "        [ 2.0775,  1.4182],\n",
      "        [ 2.1578,  2.4952],\n",
      "        [-1.7206, -1.8963],\n",
      "        [-2.0035, -1.3354],\n",
      "        [-3.6180, -3.1755],\n",
      "        [-1.6863, -1.3531],\n",
      "        [ 1.5082,  0.7324],\n",
      "        [ 1.5442,  2.3129],\n",
      "        [-0.2166,  0.5430],\n",
      "        [ 3.0160,  2.6457],\n",
      "        [-0.4779, -1.2354],\n",
      "        [ 0.8176,  0.0879],\n",
      "        [ 2.2390,  2.0735],\n",
      "        [-3.8921, -3.9501],\n",
      "        [ 2.0740,  1.7341],\n",
      "        [-1.6084, -1.2066],\n",
      "        [ 1.6059,  1.0487],\n",
      "        [ 2.2169,  2.4680],\n",
      "        [ 0.6261,  1.3211],\n",
      "        [ 0.3809, -0.2381],\n",
      "        [-2.2950, -1.7580],\n",
      "        [ 1.3648,  1.5790],\n",
      "        [-1.7894, -1.6269],\n",
      "        [ 2.0829,  3.2186],\n",
      "        [-1.5756, -1.9131],\n",
      "        [-1.8000, -1.7756],\n",
      "        [ 1.6879,  2.3890],\n",
      "        [-3.1820, -2.6220],\n",
      "        [-0.7694, -0.0660],\n",
      "        [ 2.0539,  1.6998],\n",
      "        [ 1.5743,  1.0071],\n",
      "        [ 0.4980, -0.2688],\n",
      "        [ 0.1462, -0.6148],\n",
      "        [ 0.9307,  0.7085],\n",
      "        [ 0.6835,  0.2705],\n",
      "        [ 1.3891,  2.2182],\n",
      "        [ 1.9657,  1.5575],\n",
      "        [-0.2333, -1.2696],\n",
      "        [ 1.4473,  1.7735],\n",
      "        [ 1.2664,  2.1355],\n",
      "        [ 0.9848,  2.2901],\n",
      "        [ 0.8803,  0.6173],\n",
      "        [ 1.4628,  1.7741],\n",
      "        [ 3.2872,  3.2339],\n",
      "        [ 2.2285,  2.0458],\n",
      "        [ 2.6260,  3.5103],\n",
      "        [-0.5946, -1.3352],\n",
      "        [ 0.7489,  2.0476],\n",
      "        [-1.1911, -1.7631],\n",
      "        [ 1.5899,  1.7187],\n",
      "        [-0.9611, -0.2960],\n",
      "        [ 1.1825,  1.6893],\n",
      "        [-1.3371, -1.8381],\n",
      "        [-0.4779, -1.2354],\n",
      "        [ 1.5644,  1.4205]], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "20 \t training  7.975958228104343\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 3.0917,  3.5022],\n",
      "        [-2.0488, -1.6973],\n",
      "        [-3.6466, -3.8546],\n",
      "        [ 1.5118,  1.1371],\n",
      "        [ 3.1010,  3.4871],\n",
      "        [-2.4315, -2.6355],\n",
      "        [ 0.5650,  0.5702],\n",
      "        [ 2.0922,  2.0433],\n",
      "        [-3.2314, -4.1019],\n",
      "        [ 1.7695,  1.3102],\n",
      "        [ 2.1193,  2.4759],\n",
      "        [-1.6269, -1.8556],\n",
      "        [-1.7193, -1.2311],\n",
      "        [-3.2858, -3.0515],\n",
      "        [-1.4899, -1.2795],\n",
      "        [ 1.2037,  0.6266],\n",
      "        [ 1.6390,  2.3399],\n",
      "        [-0.0177,  0.6123],\n",
      "        [ 2.7429,  2.5486],\n",
      "        [-0.6010, -1.2711],\n",
      "        [ 0.6074,  0.0177],\n",
      "        [ 2.0791,  2.0151],\n",
      "        [-3.6466, -3.8546],\n",
      "        [ 1.8815,  1.6658],\n",
      "        [-1.4026, -1.1301],\n",
      "        [ 1.3874,  0.9733],\n",
      "        [ 2.1553,  2.4411],\n",
      "        [ 0.7607,  1.3652],\n",
      "        [ 0.0260, -0.3682],\n",
      "        [-2.0222, -1.6571],\n",
      "        [ 0.9941,  1.4351],\n",
      "        [-1.6215, -1.5621],\n",
      "        [ 2.2277,  3.2603],\n",
      "        [-1.5259, -1.8871],\n",
      "        [-1.6597, -1.7196],\n",
      "        [ 1.7590,  2.4075],\n",
      "        [-2.8358, -2.4923],\n",
      "        [-0.5500,  0.0122],\n",
      "        [ 1.8591,  1.6308],\n",
      "        [ 1.3552,  0.9316],\n",
      "        [ 0.3010, -0.3333],\n",
      "        [-0.2020, -0.7406],\n",
      "        [ 0.5650,  0.5702],\n",
      "        [ 0.3220,  0.1356],\n",
      "        [ 1.5067,  2.2535],\n",
      "        [ 1.7628,  1.4862],\n",
      "        [-0.4643, -1.3437],\n",
      "        [ 1.4515,  1.7710],\n",
      "        [ 1.4003,  2.1768],\n",
      "        [ 1.2419,  2.3765],\n",
      "        [ 0.5154,  0.4796],\n",
      "        [ 1.4627,  1.7702],\n",
      "        [ 3.0830,  3.1582],\n",
      "        [ 2.0652,  1.9864],\n",
      "        [ 2.6799,  3.5192],\n",
      "        [-0.7051, -1.3662],\n",
      "        [ 1.0218,  2.1410],\n",
      "        [-1.2195, -1.7643],\n",
      "        [ 1.5413,  1.6985],\n",
      "        [-0.7386, -0.2160],\n",
      "        [ 1.2426,  1.7063],\n",
      "        [-1.3394, -1.8300],\n",
      "        [-0.6010, -1.2711],\n",
      "        [ 1.4550,  1.3809]], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "30 \t training  7.4260000965055895\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 2.9056,  3.4250],\n",
      "        [-1.8015, -1.5816],\n",
      "        [-3.3594, -3.7213],\n",
      "        [ 1.3319,  1.0656],\n",
      "        [ 2.9105,  3.4082],\n",
      "        [-2.2241, -2.5339],\n",
      "        [ 0.2817,  0.4535],\n",
      "        [ 1.9210,  1.9742],\n",
      "        [-3.0518, -4.0084],\n",
      "        [ 1.5268,  1.2110],\n",
      "        [ 2.0105,  2.4319],\n",
      "        [-1.4875, -1.7830],\n",
      "        [-1.4740, -1.1173],\n",
      "        [-2.9779, -2.9103],\n",
      "        [-1.3016, -1.1882],\n",
      "        [ 0.9782,  0.5361],\n",
      "        [ 1.6341,  2.3391],\n",
      "        [ 0.1160,  0.6753],\n",
      "        [ 2.4905,  2.4448],\n",
      "        [-0.6371, -1.2745],\n",
      "        [ 0.4648, -0.0354],\n",
      "        [ 1.9066,  1.9455],\n",
      "        [-3.3594, -3.7213],\n",
      "        [ 1.6999,  1.5928],\n",
      "        [-1.2125, -1.0385],\n",
      "        [ 1.2104,  0.9033],\n",
      "        [ 2.0319,  2.3911],\n",
      "        [ 0.8200,  1.3946],\n",
      "        [-0.2310, -0.4701],\n",
      "        [-1.7747, -1.5415],\n",
      "        [ 0.6937,  1.3083],\n",
      "        [-1.4431, -1.4743],\n",
      "        [ 2.2347,  3.2608],\n",
      "        [-1.4151, -1.8263],\n",
      "        [-1.4941, -1.6366],\n",
      "        [ 1.7332,  2.3978],\n",
      "        [-2.5280, -2.3517],\n",
      "        [-0.3831,  0.0910],\n",
      "        [ 1.6771,  1.5577],\n",
      "        [ 1.1791,  0.8621],\n",
      "        [ 0.1790, -0.3766],\n",
      "        [-0.4453, -0.8351],\n",
      "        [ 0.2817,  0.4535],\n",
      "        [ 0.0495,  0.0252],\n",
      "        [ 1.5230,  2.2617],\n",
      "        [ 1.5802,  1.4130],\n",
      "        [-0.5823, -1.3821],\n",
      "        [ 1.3980,  1.7518],\n",
      "        [ 1.4324,  2.1918],\n",
      "        [ 1.3820,  2.4337],\n",
      "        [ 0.2342,  0.3641],\n",
      "        [ 1.4062,  1.7497],\n",
      "        [ 2.8492,  3.0622],\n",
      "        [ 1.8914,  1.9162],\n",
      "        [ 2.6025,  3.4851],\n",
      "        [-0.7285, -1.3638],\n",
      "        [ 1.1840,  2.2080],\n",
      "        [-1.1683, -1.7288],\n",
      "        [ 1.4530,  1.6648],\n",
      "        [-0.5629, -0.1329],\n",
      "        [ 1.2326,  1.7055],\n",
      "        [-1.2671, -1.7854],\n",
      "        [-0.6371, -1.2745],\n",
      "        [ 1.3365,  1.3351]], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "40 \t training  6.788821324623786\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 2.5488,  3.2827],\n",
      "        [-1.5589, -1.4427],\n",
      "        [-3.0004, -3.5389],\n",
      "        [ 1.1243,  0.9764],\n",
      "        [ 2.5533,  3.2653],\n",
      "        [-1.9435, -2.3855],\n",
      "        [ 0.0527,  0.3582],\n",
      "        [ 1.6616,  1.8666],\n",
      "        [-2.6920, -3.8363],\n",
      "        [ 1.2890,  1.1037],\n",
      "        [ 1.7554,  2.3325],\n",
      "        [-1.2883, -1.6717],\n",
      "        [-1.2540, -0.9882],\n",
      "        [-2.6691, -2.7402],\n",
      "        [-1.1086, -1.0742],\n",
      "        [ 0.7790,  0.4471],\n",
      "        [ 1.4640,  2.2810],\n",
      "        [ 0.1654,  0.7223],\n",
      "        [ 2.1864,  2.3114],\n",
      "        [-0.6035, -1.2477],\n",
      "        [ 0.3430, -0.0852],\n",
      "        [ 1.6484,  1.8382],\n",
      "        [-3.0004, -3.5389],\n",
      "        [ 1.4599,  1.4911],\n",
      "        [-1.0268, -0.9272],\n",
      "        [ 1.0137,  0.8188],\n",
      "        [ 1.7704,  2.2880],\n",
      "        [ 0.7629,  1.3898],\n",
      "        [-0.4115, -0.5413],\n",
      "        [-1.5340, -1.4033],\n",
      "        [ 0.4320,  1.1965],\n",
      "        [-1.2393, -1.3570],\n",
      "        [ 2.0222,  3.1834],\n",
      "        [-1.2323, -1.7243],\n",
      "        [-1.2880, -1.5198],\n",
      "        [ 1.5418,  2.3298],\n",
      "        [-2.2388, -2.1902],\n",
      "        [-0.2747,  0.1667],\n",
      "        [ 1.4391,  1.4567],\n",
      "        [ 0.9853,  0.7788],\n",
      "        [ 0.0913, -0.4098],\n",
      "        [-0.6016, -0.8945],\n",
      "        [ 0.0527,  0.3582],\n",
      "        [-0.1589, -0.0598],\n",
      "        [ 1.3761,  2.2145],\n",
      "        [ 1.3507,  1.3151],\n",
      "        [-0.6091, -1.3860],\n",
      "        [ 1.2247,  1.6895],\n",
      "        [ 1.3039,  2.1533],\n",
      "        [ 1.3477,  2.4350],\n",
      "        [ 0.0093,  0.2709],\n",
      "        [ 1.2308,  1.6863],\n",
      "        [ 2.5063,  2.9191],\n",
      "        [ 1.6345,  1.8093],\n",
      "        [ 2.3049,  3.3712],\n",
      "        [-0.6788, -1.3291],\n",
      "        [ 1.1842,  2.2249],\n",
      "        [-1.0356, -1.6527],\n",
      "        [ 1.2601,  1.5915],\n",
      "        [-0.4360, -0.0483],\n",
      "        [ 1.0969,  1.6623],\n",
      "        [-1.1146, -1.6994],\n",
      "        [-0.6035, -1.2477],\n",
      "        [ 1.1450,  1.2591]], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "50 \t training  5.882041494260815\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 1.9284,  3.0410],\n",
      "        [-1.2598, -1.2804],\n",
      "        [-2.4956, -3.3014],\n",
      "        [ 0.8411,  0.8529],\n",
      "        [ 1.9391,  3.0256],\n",
      "        [-1.4957, -2.1722],\n",
      "        [-0.0914,  0.3067],\n",
      "        [ 1.2378,  1.6905],\n",
      "        [-2.0222, -3.5538],\n",
      "        [ 1.0176,  0.9798],\n",
      "        [ 1.2431,  2.1282],\n",
      "        [-0.9371, -1.4989],\n",
      "        [-1.0127, -0.8504],\n",
      "        [-2.2923, -2.5416],\n",
      "        [-0.8498, -0.9335],\n",
      "        [ 0.5876,  0.3595],\n",
      "        [ 0.9902,  2.0937],\n",
      "        [ 0.0938,  0.7137],\n",
      "        [ 1.7704,  2.1307],\n",
      "        [-0.4237, -1.1594],\n",
      "        [ 0.2383, -0.1302],\n",
      "        [ 1.2295,  1.6637],\n",
      "        [-2.4956, -3.3014],\n",
      "        [ 1.0977,  1.3367],\n",
      "        [-0.7907, -0.7957],\n",
      "        [ 0.7548,  0.7053],\n",
      "        [ 1.2667,  2.0863],\n",
      "        [ 0.5013,  1.2929],\n",
      "        [-0.4595, -0.5497],\n",
      "        [-1.2400, -1.2431],\n",
      "        [ 0.2250,  1.1174],\n",
      "        [-0.9367, -1.1996],\n",
      "        [ 1.4019,  2.9393],\n",
      "        [-0.8777, -1.5526],\n",
      "        [-0.9597, -1.3536],\n",
      "        [ 1.0483,  2.1345],\n",
      "        [-1.9135, -2.0133],\n",
      "        [-0.2246,  0.2151],\n",
      "        [ 1.0824,  1.3043],\n",
      "        [ 0.7325,  0.6679],\n",
      "        [ 0.0533, -0.4239],\n",
      "        [-0.6014, -0.8814],\n",
      "        [-0.0914,  0.3067],\n",
      "        [-0.2626, -0.0932],\n",
      "        [ 0.9279,  2.0376],\n",
      "        [ 1.0158,  1.1710],\n",
      "        [-0.4646, -1.3167],\n",
      "        [ 0.8319,  1.5326],\n",
      "        [ 0.8785,  1.9858],\n",
      "        [ 0.9773,  2.2867],\n",
      "        [-0.1270,  0.2229],\n",
      "        [ 0.8374,  1.5290],\n",
      "        [ 1.9861,  2.7065],\n",
      "        [ 1.2205,  1.6365],\n",
      "        [ 1.6284,  3.1102],\n",
      "        [-0.4735, -1.2288],\n",
      "        [ 0.8754,  2.1027],\n",
      "        [-0.7185, -1.5002],\n",
      "        [ 0.8738,  1.4351],\n",
      "        [-0.3451,  0.0191],\n",
      "        [ 0.7316,  1.5185],\n",
      "        [-0.7784, -1.5377],\n",
      "        [-0.4237, -1.1594],\n",
      "        [ 0.8115,  1.1210]], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "60 \t training  4.4908374616416245\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 1.1264,  2.6758],\n",
      "        [-0.8380, -1.0529],\n",
      "        [-1.8284, -2.9766],\n",
      "        [ 0.5098,  0.6585],\n",
      "        [ 1.1486,  2.6634],\n",
      "        [-0.8078, -1.8506],\n",
      "        [ 0.0462,  0.3249],\n",
      "        [ 0.6598,  1.4001],\n",
      "        [-1.0605, -3.1517],\n",
      "        [ 0.7520,  0.8017],\n",
      "        [ 0.4635,  1.7738],\n",
      "        [-0.2976, -1.2041],\n",
      "        [-0.7006, -0.6706],\n",
      "        [-1.7937, -2.2701],\n",
      "        [-0.4371, -0.7214],\n",
      "        [ 0.4908,  0.2605],\n",
      "        [ 0.0866,  1.7004],\n",
      "        [-0.2027,  0.5909],\n",
      "        [ 1.2613,  1.8519],\n",
      "        [ 0.1744, -0.9060],\n",
      "        [ 0.2731, -0.1543],\n",
      "        [ 0.6601,  1.3764],\n",
      "        [-1.8284, -2.9766],\n",
      "        [ 0.6258,  1.0848],\n",
      "        [-0.4242, -0.6029],\n",
      "        [ 0.4692,  0.5309],\n",
      "        [ 0.5197,  1.7418],\n",
      "        [-0.1221,  1.0132],\n",
      "        [-0.0716, -0.4115],\n",
      "        [-0.8275, -1.0194],\n",
      "        [ 0.2071,  1.0621],\n",
      "        [-0.4320, -0.9515],\n",
      "        [ 0.2593,  2.4685],\n",
      "        [-0.1887, -1.2425],\n",
      "        [-0.3945, -1.0837],\n",
      "        [ 0.1547,  1.7444],\n",
      "        [-1.5037, -1.7788],\n",
      "        [-0.2564,  0.2225],\n",
      "        [ 0.6197,  1.0560],\n",
      "        [ 0.4589,  0.4989],\n",
      "        [ 0.2346, -0.3759],\n",
      "        [-0.0888, -0.6838],\n",
      "        [ 0.0462,  0.3249],\n",
      "        [-0.0203, -0.0248],\n",
      "        [ 0.0234,  1.6443],\n",
      "        [ 0.5912,  0.9377],\n",
      "        [ 0.1979, -1.0453],\n",
      "        [ 0.1521,  1.2186],\n",
      "        [-0.0197,  1.5950],\n",
      "        [-0.0765,  1.8363],\n",
      "        [ 0.0309,  0.2508],\n",
      "        [ 0.1614,  1.2161],\n",
      "        [ 1.3372,  2.3832],\n",
      "        [ 0.6598,  1.3521],\n",
      "        [ 0.5968,  2.6789],\n",
      "        [ 0.1596, -0.9585],\n",
      "        [-0.1026,  1.6808],\n",
      "        [ 0.0064, -1.1829],\n",
      "        [ 0.2577,  1.1416],\n",
      "        [-0.2865,  0.0704],\n",
      "        [ 0.0267,  1.1989],\n",
      "        [-0.0564, -1.2192],\n",
      "        [ 0.1744, -0.9060],\n",
      "        [ 0.3267,  0.8747]], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "70 \t training  2.7672670994799766\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 6.3182e-01,  2.2175e+00],\n",
      "        [-3.9286e-01, -6.6827e-01],\n",
      "        [-1.2951e+00, -2.5107e+00],\n",
      "        [ 1.8164e-01,  3.2590e-01],\n",
      "        [ 6.5597e-01,  2.2051e+00],\n",
      "        [-2.9083e-01, -1.4261e+00],\n",
      "        [-1.2138e-01,  7.7755e-02],\n",
      "        [ 2.3789e-01,  1.0014e+00],\n",
      "        [-4.8069e-01, -2.6866e+00],\n",
      "        [ 4.0953e-01,  4.3676e-01],\n",
      "        [ 1.3545e-03,  1.3658e+00],\n",
      "        [ 1.6647e-01, -8.4272e-01],\n",
      "        [-3.0180e-01, -3.2188e-01],\n",
      "        [-1.2943e+00, -1.8196e+00],\n",
      "        [-3.4827e-02, -3.8979e-01],\n",
      "        [ 2.3917e-01, -3.0460e-02],\n",
      "        [-3.6932e-01,  1.3312e+00],\n",
      "        [-2.3036e-01,  5.8071e-01],\n",
      "        [ 8.2607e-01,  1.4149e+00],\n",
      "        [ 4.8797e-01, -7.1482e-01],\n",
      "        [ 1.6273e-01, -3.1296e-01],\n",
      "        [ 2.4058e-01,  9.7896e-01],\n",
      "        [-1.2951e+00, -2.5107e+00],\n",
      "        [ 2.3790e-01,  7.0762e-01],\n",
      "        [-4.1115e-02, -2.8510e-01],\n",
      "        [ 1.6395e-01,  2.1620e-01],\n",
      "        [ 6.2062e-02,  1.3324e+00],\n",
      "        [-4.0175e-01,  7.9273e-01],\n",
      "        [-6.0296e-02, -5.1566e-01],\n",
      "        [-3.8583e-01, -6.3740e-01],\n",
      "        [-6.2492e-02,  7.3083e-01],\n",
      "        [ 2.5574e-03, -5.9902e-01],\n",
      "        [-2.8069e-01,  2.0417e+00],\n",
      "        [ 2.7739e-01, -8.8929e-01],\n",
      "        [ 5.6201e-02, -7.2340e-01],\n",
      "        [-3.0826e-01,  1.3632e+00],\n",
      "        [-1.0297e+00, -1.3475e+00],\n",
      "        [-9.7689e-02,  3.6323e-01],\n",
      "        [ 2.3521e-01,  6.8121e-01],\n",
      "        [ 1.5988e-01,  1.8905e-01],\n",
      "        [ 2.2818e-01, -4.5046e-01],\n",
      "        [ 1.5623e-02, -7.1462e-01],\n",
      "        [-1.2138e-01,  7.7755e-02],\n",
      "        [-1.1495e-01, -2.1318e-01],\n",
      "        [-4.2126e-01,  1.2903e+00],\n",
      "        [ 2.2161e-01,  5.7365e-01],\n",
      "        [ 4.8283e-01, -9.0227e-01],\n",
      "        [-2.3903e-01,  8.8058e-01],\n",
      "        [-4.5262e-01,  1.2547e+00],\n",
      "        [-5.2798e-01,  1.5194e+00],\n",
      "        [-1.2277e-01,  1.4940e-02],\n",
      "        [-2.3016e-01,  8.7682e-01],\n",
      "        [ 8.6966e-01,  1.9292e+00],\n",
      "        [ 2.4273e-01,  9.5610e-01],\n",
      "        [ 6.7404e-02,  2.2302e+00],\n",
      "        [ 5.0205e-01, -7.4172e-01],\n",
      "        [-5.0562e-01,  1.4057e+00],\n",
      "        [ 4.4881e-01, -8.6739e-01],\n",
      "        [-1.2947e-01,  7.9526e-01],\n",
      "        [-6.9464e-02,  2.5802e-01],\n",
      "        [-3.4570e-01,  8.8851e-01],\n",
      "        [ 3.9877e-01, -8.8714e-01],\n",
      "        [ 4.8797e-01, -7.1482e-01],\n",
      "        [-2.4910e-02,  5.4465e-01]], dtype=torch.float64,\n",
      "       grad_fn=<SubBackward0>)\n",
      "80 \t training  1.6838739825297704\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 0.4423,  1.8496],\n",
      "        [-0.2409, -0.3596],\n",
      "        [-1.0336, -2.0783],\n",
      "        [ 0.0980,  0.1019],\n",
      "        [ 0.4663,  1.8370],\n",
      "        [-0.0901, -1.0596],\n",
      "        [-0.0432,  0.0172],\n",
      "        [ 0.1020,  0.7045],\n",
      "        [-0.2326, -2.2658],\n",
      "        [ 0.3210,  0.1925],\n",
      "        [-0.1434,  1.0593],\n",
      "        [ 0.3016, -0.5566],\n",
      "        [-0.1898, -0.0618],\n",
      "        [-1.0616, -1.4179],\n",
      "        [ 0.0724, -0.1400],\n",
      "        [ 0.2056, -0.1962],\n",
      "        [-0.4912,  1.0657],\n",
      "        [-0.2819,  0.5478],\n",
      "        [ 0.6623,  1.0757],\n",
      "        [ 0.5515, -0.5740],\n",
      "        [ 0.1616, -0.3892],\n",
      "        [ 0.1058,  0.6836],\n",
      "        [-1.0336, -2.0783],\n",
      "        [ 0.1206,  0.4356],\n",
      "        [ 0.0541, -0.0514],\n",
      "        [ 0.0920,  0.0101],\n",
      "        [-0.0838,  1.0238],\n",
      "        [-0.4800,  0.6379],\n",
      "        [ 0.0675, -0.4779],\n",
      "        [-0.2370, -0.3325],\n",
      "        [-0.0314,  0.5980],\n",
      "        [ 0.1290, -0.3241],\n",
      "        [-0.4332,  1.7248],\n",
      "        [ 0.4058, -0.6126],\n",
      "        [ 0.1901, -0.4388],\n",
      "        [-0.4359,  1.0861],\n",
      "        [-0.8328, -0.9842],\n",
      "        [-0.1083,  0.4311],\n",
      "        [ 0.1199,  0.4119],\n",
      "        [ 0.0910, -0.0123],\n",
      "        [ 0.2487, -0.4705],\n",
      "        [ 0.1555, -0.6368],\n",
      "        [-0.0432,  0.0172],\n",
      "        [-0.0117, -0.2296],\n",
      "        [-0.5372,  1.0382],\n",
      "        [ 0.1149,  0.3163],\n",
      "        [ 0.5632, -0.7788],\n",
      "        [-0.3431,  0.6418],\n",
      "        [-0.5642,  1.0139],\n",
      "        [-0.6504,  1.2932],\n",
      "        [-0.0393, -0.0367],\n",
      "        [-0.3347,  0.6369],\n",
      "        [ 0.6846,  1.5654],\n",
      "        [ 0.1092,  0.6624],\n",
      "        [-0.1067,  1.8824],\n",
      "        [ 0.5712, -0.5835],\n",
      "        [-0.6278,  1.2030],\n",
      "        [ 0.5531, -0.6295],\n",
      "        [-0.2354,  0.5496],\n",
      "        [-0.0596,  0.3635],\n",
      "        [-0.4412,  0.6721],\n",
      "        [ 0.5125, -0.6330],\n",
      "        [ 0.5515, -0.5740],\n",
      "        [-0.1182,  0.3159]], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "90 \t training  1.146945131712782\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 3.4213e-01,  1.5960e+00],\n",
      "        [-1.6462e-01, -1.5514e-01],\n",
      "        [-8.7046e-01, -1.7622e+00],\n",
      "        [ 4.9315e-02, -4.5726e-02],\n",
      "        [ 3.6505e-01,  1.5831e+00],\n",
      "        [ 1.0835e-02, -7.9891e-01],\n",
      "        [ 2.5900e-02,  5.1095e-02],\n",
      "        [ 3.4447e-02,  5.0255e-01],\n",
      "        [-1.0291e-01, -1.9590e+00],\n",
      "        [ 2.6722e-01,  4.1880e-02],\n",
      "        [-1.9754e-01,  8.5037e-01],\n",
      "        [ 3.4642e-01, -3.6030e-01],\n",
      "        [-1.3822e-01,  9.9865e-02],\n",
      "        [-9.2143e-01, -1.1377e+00],\n",
      "        [ 1.1597e-01,  2.4021e-02],\n",
      "        [ 1.7544e-01, -2.9026e-01],\n",
      "        [-5.0348e-01,  8.8832e-01],\n",
      "        [-2.7465e-01,  5.2049e-01],\n",
      "        [ 5.6439e-01,  8.4688e-01],\n",
      "        [ 5.2234e-01, -4.8767e-01],\n",
      "        [ 1.3269e-01, -4.3982e-01],\n",
      "        [ 3.8537e-02,  4.8284e-01],\n",
      "        [-8.7046e-01, -1.7622e+00],\n",
      "        [ 5.8817e-02,  2.5255e-01],\n",
      "        [ 9.2571e-02,  9.9534e-02],\n",
      "        [ 4.7598e-02, -1.2492e-01],\n",
      "        [-1.4285e-01,  8.1316e-01],\n",
      "        [-4.6906e-01,  5.3565e-01],\n",
      "        [ 1.2236e-01, -4.0284e-01],\n",
      "        [-1.6275e-01, -1.3141e-01],\n",
      "        [ 3.6510e-02,  5.9727e-01],\n",
      "        [ 1.7970e-01, -1.3865e-01],\n",
      "        [-4.5586e-01,  1.5138e+00],\n",
      "        [ 4.3913e-01, -4.2333e-01],\n",
      "        [ 2.4142e-01, -2.4462e-01],\n",
      "        [-4.5678e-01,  9.0022e-01],\n",
      "        [-7.2597e-01, -7.4701e-01],\n",
      "        [-1.0290e-01,  4.6422e-01],\n",
      "        [ 5.8873e-02,  2.3091e-01],\n",
      "        [ 4.7634e-02, -1.4398e-01],\n",
      "        [ 2.1883e-01, -4.8744e-01],\n",
      "        [ 1.9713e-01, -5.4705e-01],\n",
      "        [ 2.5900e-02,  5.1095e-02],\n",
      "        [ 5.4023e-02, -1.7611e-01],\n",
      "        [-5.4121e-01,  8.7056e-01],\n",
      "        [ 5.7240e-02,  1.4425e-01],\n",
      "        [ 5.3055e-01, -7.0144e-01],\n",
      "        [-3.6494e-01,  4.8077e-01],\n",
      "        [-5.6250e-01,  8.5435e-01],\n",
      "        [-6.3220e-01,  1.1432e+00],\n",
      "        [ 2.9526e-02,  1.2682e-03],\n",
      "        [-3.5754e-01,  4.7513e-01],\n",
      "        [ 5.7675e-01,  1.3151e+00],\n",
      "        [ 4.2202e-02,  4.6286e-01],\n",
      "        [-1.6842e-01,  1.6463e+00],\n",
      "        [ 5.4545e-01, -4.8450e-01],\n",
      "        [-6.1248e-01,  1.0659e+00],\n",
      "        [ 5.5813e-01, -4.7038e-01],\n",
      "        [-2.6841e-01,  3.8322e-01],\n",
      "        [-5.0739e-02,  4.2084e-01],\n",
      "        [-4.4787e-01,  5.2759e-01],\n",
      "        [ 5.2779e-01, -4.6122e-01],\n",
      "        [ 5.2234e-01, -4.8767e-01],\n",
      "        [-1.5640e-01,  1.6116e-01]], dtype=torch.float64,\n",
      "       grad_fn=<SubBackward0>)\n",
      "100 \t training  0.85125695121795\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 2.7952e-01,  1.4073e+00],\n",
      "        [-1.1759e-01, -1.3237e-02],\n",
      "        [-7.5897e-01, -1.5219e+00],\n",
      "        [ 7.8391e-03, -1.5814e-01],\n",
      "        [ 3.0097e-01,  1.3943e+00],\n",
      "        [ 6.9247e-02, -6.0080e-01],\n",
      "        [ 6.0575e-02,  1.1381e-01],\n",
      "        [-6.7092e-03,  3.5009e-01],\n",
      "        [-2.5084e-02, -1.7196e+00],\n",
      "        [ 2.1543e-01, -6.4696e-02],\n",
      "        [-2.1513e-01,  6.9181e-01],\n",
      "        [ 3.6200e-01, -2.1197e-01],\n",
      "        [-1.0457e-01,  2.0457e-01],\n",
      "        [-8.3080e-01, -9.3832e-01],\n",
      "        [ 1.4312e-01,  1.4152e-01],\n",
      "        [ 1.2990e-01, -3.5724e-01],\n",
      "        [-4.7532e-01,  7.5194e-01],\n",
      "        [-2.2908e-01,  5.0063e-01],\n",
      "        [ 4.9114e-01,  6.7961e-01],\n",
      "        [ 4.6490e-01, -4.2932e-01],\n",
      "        [ 8.4079e-02, -4.8590e-01],\n",
      "        [-2.9587e-03,  3.3128e-01],\n",
      "        [-7.5897e-01, -1.5219e+00],\n",
      "        [ 1.5908e-02,  1.1438e-01],\n",
      "        [ 1.1888e-01,  2.0618e-01],\n",
      "        [ 6.4220e-03, -2.2838e-01],\n",
      "        [-1.6666e-01,  6.5351e-01],\n",
      "        [-4.1846e-01,  4.5924e-01],\n",
      "        [ 1.2803e-01, -3.2470e-01],\n",
      "        [-1.1688e-01,  7.4132e-03],\n",
      "        [ 8.8529e-02,  6.4874e-01],\n",
      "        [ 2.0686e-01, -2.4631e-03],\n",
      "        [-4.3524e-01,  1.3520e+00],\n",
      "        [ 4.4283e-01, -2.7936e-01],\n",
      "        [ 2.6584e-01, -1.0000e-01],\n",
      "        [-4.3744e-01,  7.5768e-01],\n",
      "        [-6.6536e-01, -5.9339e-01],\n",
      "        [-7.1959e-02,  4.8510e-01],\n",
      "        [ 1.6000e-02,  9.4268e-02],\n",
      "        [ 6.4871e-03, -2.4511e-01],\n",
      "        [ 1.6343e-01, -5.1040e-01],\n",
      "        [ 1.8800e-01, -4.6260e-01],\n",
      "        [ 6.0575e-02,  1.1381e-01],\n",
      "        [ 7.7048e-02, -1.0613e-01],\n",
      "        [-5.0477e-01,  7.4127e-01],\n",
      "        [ 1.4680e-02,  1.4209e-02],\n",
      "        [ 4.5895e-01, -6.4937e-01],\n",
      "        [-3.5280e-01,  3.5772e-01],\n",
      "        [-5.2043e-01,  7.3098e-01],\n",
      "        [-5.7871e-01,  1.0184e+00],\n",
      "        [ 6.2021e-02,  6.5506e-02],\n",
      "        [-3.4672e-01,  3.5145e-01],\n",
      "        [ 5.0112e-01,  1.1300e+00],\n",
      "        [ 4.0830e-04,  3.1226e-01],\n",
      "        [-1.8650e-01,  1.4687e+00],\n",
      "        [ 4.9167e-01, -4.1536e-01],\n",
      "        [-5.6232e-01,  9.4889e-01],\n",
      "        [ 5.3477e-01, -3.5010e-01],\n",
      "        [-2.7160e-01,  2.5592e-01],\n",
      "        [-2.3559e-02,  4.5810e-01],\n",
      "        [-4.1772e-01,  4.1743e-01],\n",
      "        [ 5.1413e-01, -3.3065e-01],\n",
      "        [ 4.6490e-01, -4.2932e-01],\n",
      "        [-1.7349e-01,  4.2294e-02]], dtype=torch.float64,\n",
      "       grad_fn=<SubBackward0>)\n",
      "110 \t training  0.6621313171116042\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 0.2430,  1.2554],\n",
      "        [-0.0959,  0.0887],\n",
      "        [-0.6856, -1.3259],\n",
      "        [-0.0145, -0.2469],\n",
      "        [ 0.2632,  1.2426],\n",
      "        [ 0.0983, -0.4406],\n",
      "        [ 0.0652,  0.1692],\n",
      "        [-0.0218,  0.2283],\n",
      "        [ 0.0254, -1.5155],\n",
      "        [ 0.1749, -0.1475],\n",
      "        [-0.2075,  0.5626],\n",
      "        [ 0.3587, -0.0948],\n",
      "        [-0.0896,  0.2732],\n",
      "        [-0.7789, -0.7885],\n",
      "        [ 0.1546,  0.2280],\n",
      "        [ 0.0872, -0.4108],\n",
      "        [-0.4303,  0.6377],\n",
      "        [-0.1705,  0.4864],\n",
      "        [ 0.4417,  0.5481],\n",
      "        [ 0.4027, -0.3890],\n",
      "        [ 0.0425, -0.5256],\n",
      "        [-0.0185,  0.2103],\n",
      "        [-0.6856, -1.3259],\n",
      "        [-0.0032,  0.0048],\n",
      "        [ 0.1317,  0.2833],\n",
      "        [-0.0174, -0.3102],\n",
      "        [-0.1646,  0.5239],\n",
      "        [-0.3497,  0.4008],\n",
      "        [ 0.1061, -0.2621],\n",
      "        [-0.0958,  0.1064],\n",
      "        [ 0.1132,  0.7023],\n",
      "        [ 0.2151,  0.1011],\n",
      "        [-0.4050,  1.2118],\n",
      "        [ 0.4297, -0.1647],\n",
      "        [ 0.2705,  0.0121],\n",
      "        [-0.3993,  0.6389],\n",
      "        [-0.6400, -0.4905],\n",
      "        [-0.0348,  0.4982],\n",
      "        [-0.0033, -0.0140],\n",
      "        [-0.0177, -0.3251],\n",
      "        [ 0.1118, -0.5337],\n",
      "        [ 0.1549, -0.3953],\n",
      "        [ 0.0652,  0.1692],\n",
      "        [ 0.0701, -0.0478],\n",
      "        [-0.4533,  0.6325],\n",
      "        [-0.0055, -0.0887],\n",
      "        [ 0.3812, -0.6142],\n",
      "        [-0.3160,  0.2597],\n",
      "        [-0.4647,  0.6270],\n",
      "        [-0.5294,  0.9021],\n",
      "        [ 0.0643,  0.1215],\n",
      "        [-0.3111,  0.2529],\n",
      "        [ 0.4520,  0.9829],\n",
      "        [-0.0156,  0.1921],\n",
      "        [-0.1851,  1.3208],\n",
      "        [ 0.4322, -0.3657],\n",
      "        [-0.5163,  0.8385],\n",
      "        [ 0.4998, -0.2552],\n",
      "        [-0.2487,  0.1549],\n",
      "        [ 0.0062,  0.4823],\n",
      "        [-0.3655,  0.3300],\n",
      "        [ 0.4870, -0.2269],\n",
      "        [ 0.4027, -0.3890],\n",
      "        [-0.1651, -0.0510]], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "120 \t training  0.5313760997458202\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 0.2198,  1.1286],\n",
      "        [-0.0927,  0.1635],\n",
      "        [-0.6379, -1.1604],\n",
      "        [-0.0155, -0.3144],\n",
      "        [ 0.2391,  1.1161],\n",
      "        [ 0.1081, -0.3092],\n",
      "        [ 0.0455,  0.2011],\n",
      "        [-0.0178,  0.1306],\n",
      "        [ 0.0632, -1.3363],\n",
      "        [ 0.1503, -0.2129],\n",
      "        [-0.1896,  0.4551],\n",
      "        [ 0.3433, -0.0037],\n",
      "        [-0.0890,  0.3195],\n",
      "        [-0.7528, -0.6702],\n",
      "        [ 0.1527,  0.2910],\n",
      "        [ 0.0578, -0.4536],\n",
      "        [-0.3877,  0.5405],\n",
      "        [-0.1171,  0.4768],\n",
      "        [ 0.4122,  0.4419],\n",
      "        [ 0.3494, -0.3608],\n",
      "        [ 0.0208, -0.5539],\n",
      "        [-0.0148,  0.1134],\n",
      "        [-0.6379, -1.1604],\n",
      "        [-0.0014, -0.0812],\n",
      "        [ 0.1321,  0.3385],\n",
      "        [-0.0199, -0.3717],\n",
      "        [-0.1501,  0.4168],\n",
      "        [-0.2826,  0.3574],\n",
      "        [ 0.0685, -0.2234],\n",
      "        [-0.0930,  0.1788],\n",
      "        [ 0.1087,  0.7339],\n",
      "        [ 0.2090,  0.1791],\n",
      "        [-0.3865,  1.0859],\n",
      "        [ 0.4074, -0.0752],\n",
      "        [ 0.2612,  0.0979],\n",
      "        [-0.3613,  0.5380],\n",
      "        [-0.6354, -0.4164],\n",
      "        [-0.0040,  0.5066],\n",
      "        [-0.0016, -0.0989],\n",
      "        [-0.0207, -0.3851],\n",
      "        [ 0.0789, -0.5504],\n",
      "        [ 0.1108, -0.3514],\n",
      "        [ 0.0455,  0.2011],\n",
      "        [ 0.0422, -0.0136],\n",
      "        [-0.4063,  0.5400],\n",
      "        [-0.0044, -0.1686],\n",
      "        [ 0.3150, -0.5901],\n",
      "        [-0.2696,  0.1827],\n",
      "        [-0.4147,  0.5386],\n",
      "        [-0.4971,  0.7964],\n",
      "        [ 0.0430,  0.1538],\n",
      "        [-0.2655,  0.1755],\n",
      "        [ 0.4215,  0.8623],\n",
      "        [-0.0121,  0.0961],\n",
      "        [-0.1843,  1.1922],\n",
      "        [ 0.3796, -0.3303],\n",
      "        [-0.4855,  0.7388],\n",
      "        [ 0.4620, -0.1825],\n",
      "        [-0.2115,  0.0763],\n",
      "        [ 0.0290,  0.4980],\n",
      "        [-0.3087,  0.2618],\n",
      "        [ 0.4547, -0.1465],\n",
      "        [ 0.3494, -0.3608],\n",
      "        [-0.1375, -0.1213]], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "130 \t training  0.43945065549521295\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 2.0218e-01,  1.0219e+00],\n",
      "        [-1.0138e-01,  2.1947e-01],\n",
      "        [-6.0781e-01, -1.0196e+00],\n",
      "        [-1.2194e-03, -3.6306e-01],\n",
      "        [ 2.2115e-01,  1.0097e+00],\n",
      "        [ 1.0368e-01, -2.0293e-01],\n",
      "        [ 1.7045e-02,  2.1640e-01],\n",
      "        [-3.4814e-03,  5.2987e-02],\n",
      "        [ 9.1288e-02, -1.1806e+00],\n",
      "        [ 1.4082e-01, -2.6282e-01],\n",
      "        [-1.7069e-01,  3.6595e-01],\n",
      "        [ 3.1832e-01,  6.4083e-02],\n",
      "        [-9.7260e-02,  3.5238e-01],\n",
      "        [-7.4217e-01, -5.7417e-01],\n",
      "        [ 1.4033e-01,  3.3586e-01],\n",
      "        [ 4.3588e-02, -4.8469e-01],\n",
      "        [-3.5444e-01,  4.5877e-01],\n",
      "        [-7.5514e-02,  4.7034e-01],\n",
      "        [ 3.9751e-01,  3.5614e-01],\n",
      "        [ 3.0568e-01, -3.4145e-01],\n",
      "        [ 1.6249e-02, -5.7020e-01],\n",
      "        [-4.1503e-04,  3.6615e-02],\n",
      "        [-6.0781e-01, -1.0196e+00],\n",
      "        [ 1.3641e-02, -1.4721e-01],\n",
      "        [ 1.2245e-01,  3.7736e-01],\n",
      "        [-6.6605e-03, -4.1505e-01],\n",
      "        [-1.3288e-01,  3.2840e-01],\n",
      "        [-2.2621e-01,  3.2559e-01],\n",
      "        [ 2.9418e-02, -2.0046e-01],\n",
      "        [-1.0179e-01,  2.3261e-01],\n",
      "        [ 8.9745e-02,  7.4789e-01],\n",
      "        [ 1.9190e-01,  2.3592e-01],\n",
      "        [-3.8229e-01,  9.7455e-01],\n",
      "        [ 3.7744e-01, -8.9292e-03],\n",
      "        [ 2.4099e-01,  1.6135e-01],\n",
      "        [-3.3114e-01,  4.5306e-01],\n",
      "        [-6.4016e-01, -3.5879e-01],\n",
      "        [ 1.7088e-02,  5.1192e-01],\n",
      "        [ 1.3415e-02, -1.6377e-01],\n",
      "        [-7.8323e-03, -4.2712e-01],\n",
      "        [ 6.3248e-02, -5.5890e-01],\n",
      "        [ 6.8339e-02, -3.2291e-01],\n",
      "        [ 1.7045e-02,  2.1640e-01],\n",
      "        [ 8.6853e-03,  4.3501e-03],\n",
      "        [-3.7009e-01,  4.6244e-01],\n",
      "        [ 1.0737e-02, -2.2876e-01],\n",
      "        [ 2.6349e-01, -5.7267e-01],\n",
      "        [-2.2455e-01,  1.2306e-01],\n",
      "        [-3.7628e-01,  4.6481e-01],\n",
      "        [-4.7900e-01,  7.0476e-01],\n",
      "        [ 1.3486e-02,  1.6950e-01],\n",
      "        [-2.2082e-01,  1.1567e-01],\n",
      "        [ 4.0247e-01,  7.6281e-01],\n",
      "        [ 2.3243e-03,  2.0122e-02],\n",
      "        [-1.9037e-01,  1.0804e+00],\n",
      "        [ 3.3488e-01, -3.0612e-01],\n",
      "        [-4.6640e-01,  6.5392e-01],\n",
      "        [ 4.2224e-01, -1.3036e-01],\n",
      "        [-1.7107e-01,  1.6178e-02],\n",
      "        [ 4.2570e-02,  5.0831e-01],\n",
      "        [-2.5779e-01,  2.0922e-01],\n",
      "        [ 4.1826e-01, -8.8085e-02],\n",
      "        [ 3.0568e-01, -3.4145e-01],\n",
      "        [-1.0112e-01, -1.7235e-01]], dtype=torch.float64,\n",
      "       grad_fn=<SubBackward0>)\n",
      "140 \t training  0.37469965499548774\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 0.1866,  0.9321],\n",
      "        [-0.1152,  0.2620],\n",
      "        [-0.5887, -0.9006],\n",
      "        [ 0.0202, -0.3977],\n",
      "        [ 0.2056,  0.9203],\n",
      "        [ 0.0905, -0.1184],\n",
      "        [-0.0107,  0.2237],\n",
      "        [ 0.0144, -0.0088],\n",
      "        [ 0.1115, -1.0477],\n",
      "        [ 0.1412, -0.3003],\n",
      "        [-0.1546,  0.2920],\n",
      "        [ 0.2880,  0.1127],\n",
      "        [-0.1083,  0.3772],\n",
      "        [-0.7389, -0.4953],\n",
      "        [ 0.1227,  0.3675],\n",
      "        [ 0.0399, -0.5063],\n",
      "        [-0.3304,  0.3904],\n",
      "        [-0.0440,  0.4662],\n",
      "        [ 0.3915,  0.2867],\n",
      "        [ 0.2692, -0.3291],\n",
      "        [ 0.0212, -0.5783],\n",
      "        [ 0.0177, -0.0244],\n",
      "        [-0.5887, -0.9006],\n",
      "        [ 0.0342, -0.1978],\n",
      "        [ 0.1078,  0.4046],\n",
      "        [ 0.0141, -0.4450],\n",
      "        [-0.1172,  0.2554],\n",
      "        [-0.1813,  0.3018],\n",
      "        [-0.0039, -0.1851],\n",
      "        [-0.1155,  0.2735],\n",
      "        [ 0.0669,  0.7525],\n",
      "        [ 0.1690,  0.2766],\n",
      "        [-0.3884,  0.8776],\n",
      "        [ 0.3433,  0.0381],\n",
      "        [ 0.2149,  0.2070],\n",
      "        [-0.3092,  0.3820],\n",
      "        [-0.6469, -0.3117],\n",
      "        [ 0.0313,  0.5159],\n",
      "        [ 0.0342, -0.2133],\n",
      "        [ 0.0126, -0.4559],\n",
      "        [ 0.0578, -0.5619],\n",
      "        [ 0.0334, -0.3024],\n",
      "        [-0.0107,  0.2237],\n",
      "        [-0.0217,  0.0146],\n",
      "        [-0.3437,  0.3978],\n",
      "        [ 0.0321, -0.2739],\n",
      "        [ 0.2238, -0.5597],\n",
      "        [-0.1853,  0.0767],\n",
      "        [-0.3482,  0.4035],\n",
      "        [-0.4698,  0.6271],\n",
      "        [-0.0148,  0.1773],\n",
      "        [-0.1818,  0.0691],\n",
      "        [ 0.3897,  0.6804],\n",
      "        [ 0.0206, -0.0400],\n",
      "        [-0.2026,  0.9841],\n",
      "        [ 0.2963, -0.2908],\n",
      "        [-0.4545,  0.5833],\n",
      "        [ 0.3820, -0.0951],\n",
      "        [-0.1335, -0.0299],\n",
      "        [ 0.0502,  0.5156],\n",
      "        [-0.2155,  0.1685],\n",
      "        [ 0.3798, -0.0478],\n",
      "        [ 0.2692, -0.3291],\n",
      "        [-0.0640, -0.2094]], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "150 \t training  0.32881249245034944\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 0.1723,  0.8566],\n",
      "        [-0.1301,  0.2947],\n",
      "        [-0.5761, -0.8003],\n",
      "        [ 0.0441, -0.4226],\n",
      "        [ 0.1916,  0.8453],\n",
      "        [ 0.0731, -0.0517],\n",
      "        [-0.0347,  0.2271],\n",
      "        [ 0.0326, -0.0584],\n",
      "        [ 0.1258, -0.9353],\n",
      "        [ 0.1471, -0.3286],\n",
      "        [-0.1416,  0.2307],\n",
      "        [ 0.2563,  0.1468],\n",
      "        [-0.1191,  0.3965],\n",
      "        [-0.7388, -0.4302],\n",
      "        [ 0.1039,  0.3898],\n",
      "        [ 0.0426, -0.5212],\n",
      "        [-0.3129,  0.3333],\n",
      "        [-0.0195,  0.4634],\n",
      "        [ 0.3903,  0.2301],\n",
      "        [ 0.2388, -0.3221],\n",
      "        [ 0.0307, -0.5817],\n",
      "        [ 0.0363, -0.0732],\n",
      "        [-0.5761, -0.8003],\n",
      "        [ 0.0563, -0.2370],\n",
      "        [ 0.0920,  0.4237],\n",
      "        [ 0.0374, -0.4657],\n",
      "        [-0.1039,  0.1951],\n",
      "        [-0.1455,  0.2837],\n",
      "        [-0.0303, -0.1737],\n",
      "        [-0.1301,  0.3049],\n",
      "        [ 0.0448,  0.7524],\n",
      "        [ 0.1447,  0.3053],\n",
      "        [-0.4000,  0.7942],\n",
      "        [ 0.3085,  0.0703],\n",
      "        [ 0.1873,  0.2391],\n",
      "        [-0.2933,  0.3225],\n",
      "        [-0.6526, -0.2726],\n",
      "        [ 0.0414,  0.5191],\n",
      "        [ 0.0565, -0.2515],\n",
      "        [ 0.0357, -0.4757],\n",
      "        [ 0.0578, -0.5620],\n",
      "        [ 0.0062, -0.2863],\n",
      "        [-0.0347,  0.2271],\n",
      "        [-0.0469,  0.0213],\n",
      "        [-0.3245,  0.3439],\n",
      "        [ 0.0553, -0.3081],\n",
      "        [ 0.1929, -0.5505],\n",
      "        [-0.1521,  0.0403],\n",
      "        [-0.3274,  0.3525],\n",
      "        [-0.4661,  0.5613],\n",
      "        [-0.0391,  0.1813],\n",
      "        [-0.1486,  0.0326],\n",
      "        [ 0.3805,  0.6119],\n",
      "        [ 0.0396, -0.0880],\n",
      "        [-0.2179,  0.9019],\n",
      "        [ 0.2634, -0.2820],\n",
      "        [-0.4474,  0.5242],\n",
      "        [ 0.3437, -0.0726],\n",
      "        [-0.1003, -0.0656],\n",
      "        [ 0.0548,  0.5210],\n",
      "        [-0.1808,  0.1365],\n",
      "        [ 0.3422, -0.0213],\n",
      "        [ 0.2388, -0.3221],\n",
      "        [-0.0292, -0.2365]], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "160 \t training  0.29595572097970074\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 1.5937e-01,  7.9348e-01],\n",
      "        [-1.4399e-01,  3.1994e-01],\n",
      "        [-5.6735e-01, -7.1588e-01],\n",
      "        [ 6.7865e-02, -4.4039e-01],\n",
      "        [ 1.7896e-01,  7.8250e-01],\n",
      "        [ 5.4319e-02,  7.2502e-04],\n",
      "        [-5.4822e-02,  2.2839e-01],\n",
      "        [ 5.0059e-02, -9.8194e-02],\n",
      "        [ 1.3615e-01, -8.4030e-01],\n",
      "        [ 1.5582e-01, -3.5008e-01],\n",
      "        [-1.3090e-01,  1.7978e-01],\n",
      "        [ 2.2622e-01,  1.7024e-01],\n",
      "        [-1.2850e-01,  4.1166e-01],\n",
      "        [-7.3955e-01, -3.7636e-01],\n",
      "        [ 8.6208e-02,  4.0534e-01],\n",
      "        [ 4.8578e-02, -5.3150e-01],\n",
      "        [-2.9998e-01,  2.8570e-01],\n",
      "        [-2.7065e-04,  4.6140e-01],\n",
      "        [ 3.9163e-01,  1.8400e-01],\n",
      "        [ 2.1403e-01, -3.1852e-01],\n",
      "        [ 4.2261e-02, -5.8238e-01],\n",
      "        [ 5.4063e-02, -1.1229e-01],\n",
      "        [-5.6735e-01, -7.1588e-01],\n",
      "        [ 7.7909e-02, -2.6735e-01],\n",
      "        [ 7.7078e-02,  4.3706e-01],\n",
      "        [ 6.0821e-02, -4.8008e-01],\n",
      "        [-9.2588e-02,  1.4524e-01],\n",
      "        [-1.1674e-01,  2.6954e-01],\n",
      "        [-5.0956e-02, -1.6491e-01],\n",
      "        [-1.4367e-01,  3.2915e-01],\n",
      "        [ 2.4488e-02,  7.4962e-01],\n",
      "        [ 1.2164e-01,  3.2534e-01],\n",
      "        [-4.1383e-01,  7.2289e-01],\n",
      "        [ 2.7583e-01,  9.1902e-02],\n",
      "        [ 1.6100e-01,  2.6161e-01],\n",
      "        [-2.8151e-01,  2.7287e-01],\n",
      "        [-6.5641e-01, -2.3958e-01],\n",
      "        [ 4.9047e-02,  5.2146e-01],\n",
      "        [ 7.8420e-02, -2.8101e-01],\n",
      "        [ 5.8988e-02, -4.8915e-01],\n",
      "        [ 6.1110e-02, -5.6077e-01],\n",
      "        [-1.4873e-02, -2.7371e-01],\n",
      "        [-5.4822e-02,  2.2839e-01],\n",
      "        [-6.7152e-02,  2.5629e-02],\n",
      "        [-3.1010e-01,  2.9898e-01],\n",
      "        [ 7.8213e-02, -3.3405e-01],\n",
      "        [ 1.6865e-01, -5.4418e-01],\n",
      "        [-1.2399e-01,  1.1510e-02],\n",
      "        [-3.1190e-01,  3.1015e-01],\n",
      "        [-4.6580e-01,  5.0536e-01],\n",
      "        [-5.9193e-02,  1.8321e-01],\n",
      "        [-1.2049e-01,  3.7403e-03],\n",
      "        [ 3.7352e-01,  5.5513e-01],\n",
      "        [ 5.7744e-02, -1.2644e-01],\n",
      "        [-2.3390e-01,  8.3222e-01],\n",
      "        [ 2.3590e-01, -2.7754e-01],\n",
      "        [-4.4354e-01,  4.7460e-01],\n",
      "        [ 3.0912e-01, -5.8895e-02],\n",
      "        [-7.1426e-02, -9.3375e-02],\n",
      "        [ 5.7835e-02,  5.2498e-01],\n",
      "        [-1.5228e-01,  1.1119e-01],\n",
      "        [ 3.0770e-01, -4.4786e-03],\n",
      "        [ 2.1403e-01, -3.1852e-01],\n",
      "        [ 2.1404e-03, -2.5644e-01]], dtype=torch.float64,\n",
      "       grad_fn=<SubBackward0>)\n",
      "170 \t training  0.2721409558413818\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 0.1479,  0.7410],\n",
      "        [-0.1562,  0.3397],\n",
      "        [-0.5606, -0.6445],\n",
      "        [ 0.0903, -0.4531],\n",
      "        [ 0.1678,  0.7303],\n",
      "        [ 0.0361,  0.0421],\n",
      "        [-0.0716,  0.2284],\n",
      "        [ 0.0660, -0.1302],\n",
      "        [ 0.1439, -0.7598],\n",
      "        [ 0.1656, -0.3664],\n",
      "        [-0.1222,  0.1376],\n",
      "        [ 0.1991,  0.1863],\n",
      "        [-0.1362,  0.4237],\n",
      "        [-0.7401, -0.3314],\n",
      "        [ 0.0705,  0.4163],\n",
      "        [ 0.0562, -0.5385],\n",
      "        [-0.2902,  0.2460],\n",
      "        [ 0.0150,  0.4598],\n",
      "        [ 0.3940,  0.1464],\n",
      "        [ 0.1942, -0.3171],\n",
      "        [ 0.0545, -0.5813],\n",
      "        [ 0.0703, -0.1437],\n",
      "        [-0.5606, -0.6445],\n",
      "        [ 0.0980, -0.2910],\n",
      "        [ 0.0639,  0.4465],\n",
      "        [ 0.0830, -0.4899],\n",
      "        [-0.0831,  0.1041],\n",
      "        [-0.0936,  0.2584],\n",
      "        [-0.0674, -0.1583],\n",
      "        [-0.1555,  0.3481],\n",
      "        [ 0.0063,  0.7454],\n",
      "        [ 0.1010,  0.3394],\n",
      "        [-0.4279,  0.6623],\n",
      "        [ 0.2466,  0.1062],\n",
      "        [ 0.1373,  0.2773],\n",
      "        [-0.2726,  0.2315],\n",
      "        [-0.6583, -0.2115],\n",
      "        [ 0.0549,  0.5232],\n",
      "        [ 0.0988, -0.3039],\n",
      "        [ 0.0811, -0.4982],\n",
      "        [ 0.0662, -0.5587],\n",
      "        [-0.0316, -0.2639],\n",
      "        [-0.0716,  0.2284],\n",
      "        [-0.0837,  0.0284],\n",
      "        [-0.2992,  0.2616],\n",
      "        [ 0.0996, -0.3538],\n",
      "        [ 0.1495, -0.5400],\n",
      "        [-0.1004, -0.0113],\n",
      "        [-0.3000,  0.2749],\n",
      "        [-0.4676,  0.4579],\n",
      "        [-0.0759,  0.1838],\n",
      "        [-0.0968, -0.0191],\n",
      "        [ 0.3679,  0.5083],\n",
      "        [ 0.0744, -0.1572],\n",
      "        [-0.2493,  0.7736],\n",
      "        [ 0.2135, -0.2757],\n",
      "        [-0.4419,  0.4327],\n",
      "        [ 0.2790, -0.0510],\n",
      "        [-0.0466, -0.1151],\n",
      "        [ 0.0600,  0.5278],\n",
      "        [-0.1288,  0.0911],\n",
      "        [ 0.2773,  0.0058],\n",
      "        [ 0.1942, -0.3171],\n",
      "        [ 0.0298, -0.2711]], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "180 \t training  0.25464248952190843\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 0.1378,  0.6975],\n",
      "        [-0.1665,  0.3553],\n",
      "        [-0.5549, -0.5838],\n",
      "        [ 0.1108, -0.4621],\n",
      "        [ 0.1581,  0.6872],\n",
      "        [ 0.0195,  0.0751],\n",
      "        [-0.0859,  0.2277],\n",
      "        [ 0.0801, -0.1560],\n",
      "        [ 0.1499, -0.6914],\n",
      "        [ 0.1754, -0.3788],\n",
      "        [-0.1149,  0.1029],\n",
      "        [ 0.1755,  0.1973],\n",
      "        [-0.1425,  0.4334],\n",
      "        [-0.7399, -0.2935],\n",
      "        [ 0.0570,  0.4240],\n",
      "        [ 0.0645, -0.5431],\n",
      "        [-0.2827,  0.2130],\n",
      "        [ 0.0268,  0.4582],\n",
      "        [ 0.3968,  0.1159],\n",
      "        [ 0.1785, -0.3168],\n",
      "        [ 0.0666, -0.5792],\n",
      "        [ 0.0848, -0.1689],\n",
      "        [-0.5549, -0.5838],\n",
      "        [ 0.1161, -0.3094],\n",
      "        [ 0.0526,  0.4531],\n",
      "        [ 0.1034, -0.4964],\n",
      "        [-0.0751,  0.0702],\n",
      "        [-0.0751,  0.2494],\n",
      "        [-0.0809, -0.1534],\n",
      "        [-0.1655,  0.3632],\n",
      "        [-0.0100,  0.7407],\n",
      "        [ 0.0831,  0.3494],\n",
      "        [-0.4411,  0.6112],\n",
      "        [ 0.2212,  0.1155],\n",
      "        [ 0.1167,  0.2883],\n",
      "        [-0.2658,  0.1971],\n",
      "        [-0.6586, -0.1874],\n",
      "        [ 0.0593,  0.5241],\n",
      "        [ 0.1172, -0.3216],\n",
      "        [ 0.1014, -0.5041],\n",
      "        [ 0.0723, -0.5560],\n",
      "        [-0.0452, -0.2565],\n",
      "        [-0.0859,  0.2277],\n",
      "        [-0.0973,  0.0302],\n",
      "        [-0.2908,  0.2306],\n",
      "        [ 0.1190, -0.3688],\n",
      "        [ 0.1345, -0.5373],\n",
      "        [-0.0807, -0.0294],\n",
      "        [-0.2909,  0.2457],\n",
      "        [-0.4704,  0.4176],\n",
      "        [-0.0900,  0.1835],\n",
      "        [-0.0771, -0.0372],\n",
      "        [ 0.3633,  0.4698],\n",
      "        [ 0.0892, -0.1819],\n",
      "        [-0.2631,  0.7245],\n",
      "        [ 0.1955, -0.2754],\n",
      "        [-0.4418,  0.3973],\n",
      "        [ 0.2533, -0.0468],\n",
      "        [-0.0255, -0.1321],\n",
      "        [ 0.0614,  0.5296],\n",
      "        [-0.1095,  0.0751],\n",
      "        [ 0.2512,  0.0119],\n",
      "        [ 0.1785, -0.3168],\n",
      "        [ 0.0539, -0.2819]], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "190 \t training  0.24157868592242396\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 0.1293,  0.6618],\n",
      "        [-0.1751,  0.3678],\n",
      "        [-0.5495, -0.5319],\n",
      "        [ 0.1290, -0.4683],\n",
      "        [ 0.1498,  0.6518],\n",
      "        [ 0.0048,  0.1015],\n",
      "        [-0.0980,  0.2267],\n",
      "        [ 0.0925, -0.1767],\n",
      "        [ 0.1549, -0.6328],\n",
      "        [ 0.1848, -0.3879],\n",
      "        [-0.1088,  0.0742],\n",
      "        [ 0.1553,  0.2047],\n",
      "        [-0.1475,  0.4411],\n",
      "        [-0.7388, -0.2612],\n",
      "        [ 0.0455,  0.4294],\n",
      "        [ 0.0728, -0.5457],\n",
      "        [-0.2768,  0.1856],\n",
      "        [ 0.0359,  0.4565],\n",
      "        [ 0.3996,  0.0912],\n",
      "        [ 0.1662, -0.3170],\n",
      "        [ 0.0781, -0.5764],\n",
      "        [ 0.0975, -0.1891],\n",
      "        [-0.5495, -0.5319],\n",
      "        [ 0.1321, -0.3236],\n",
      "        [ 0.0430,  0.4577],\n",
      "        [ 0.1216, -0.5004],\n",
      "        [-0.0682,  0.0423],\n",
      "        [-0.0604,  0.2421],\n",
      "        [-0.0921, -0.1497],\n",
      "        [-0.1738,  0.3752],\n",
      "        [-0.0244,  0.7361],\n",
      "        [ 0.0679,  0.3564],\n",
      "        [-0.4528,  0.5682],\n",
      "        [ 0.1995,  0.1215],\n",
      "        [ 0.0992,  0.2960],\n",
      "        [-0.2604,  0.1686],\n",
      "        [-0.6575, -0.1665],\n",
      "        [ 0.0624,  0.5244],\n",
      "        [ 0.1334, -0.3353],\n",
      "        [ 0.1195, -0.5077],\n",
      "        [ 0.0788, -0.5530],\n",
      "        [-0.0567, -0.2509],\n",
      "        [-0.0980,  0.2267],\n",
      "        [-0.1088,  0.0314],\n",
      "        [-0.2843,  0.2048],\n",
      "        [ 0.1361, -0.3802],\n",
      "        [ 0.1225, -0.5356],\n",
      "        [-0.0644, -0.0439],\n",
      "        [-0.2837,  0.2214],\n",
      "        [-0.4736,  0.3836],\n",
      "        [-0.1020,  0.1830],\n",
      "        [-0.0607, -0.0517],\n",
      "        [ 0.3595,  0.4383],\n",
      "        [ 0.1023, -0.2015],\n",
      "        [-0.2750,  0.6837],\n",
      "        [ 0.1812, -0.2758],\n",
      "        [-0.4424,  0.3674],\n",
      "        [ 0.2318, -0.0448],\n",
      "        [-0.0077, -0.1455],\n",
      "        [ 0.0620,  0.5306],\n",
      "        [-0.0938,  0.0621],\n",
      "        [ 0.2291,  0.0153],\n",
      "        [ 0.1662, -0.3170],\n",
      "        [ 0.0743, -0.2899]], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "200 \t training  0.23163800051930006\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 0.1225,  0.6326],\n",
      "        [-0.1820,  0.3778],\n",
      "        [-0.5440, -0.4874],\n",
      "        [ 0.1448, -0.4724],\n",
      "        [ 0.1432,  0.6228],\n",
      "        [-0.0078,  0.1228],\n",
      "        [-0.1086,  0.2258],\n",
      "        [ 0.1032, -0.1932],\n",
      "        [ 0.1592, -0.5826],\n",
      "        [ 0.1934, -0.3944],\n",
      "        [-0.1035,  0.0506],\n",
      "        [ 0.1382,  0.2098],\n",
      "        [-0.1516,  0.4472],\n",
      "        [-0.7366, -0.2336],\n",
      "        [ 0.0358,  0.4331],\n",
      "        [ 0.0807, -0.5468],\n",
      "        [-0.2720,  0.1629],\n",
      "        [ 0.0425,  0.4545],\n",
      "        [ 0.4023,  0.0715],\n",
      "        [ 0.1567, -0.3175],\n",
      "        [ 0.0886, -0.5731],\n",
      "        [ 0.1086, -0.2052],\n",
      "        [-0.5440, -0.4874],\n",
      "        [ 0.1460, -0.3346],\n",
      "        [ 0.0348,  0.4608],\n",
      "        [ 0.1375, -0.5028],\n",
      "        [-0.0623,  0.0194],\n",
      "        [-0.0489,  0.2358],\n",
      "        [-0.1018, -0.1468],\n",
      "        [-0.1804,  0.3848],\n",
      "        [-0.0373,  0.7320],\n",
      "        [ 0.0551,  0.3613],\n",
      "        [-0.4624,  0.5321],\n",
      "        [ 0.1812,  0.1253],\n",
      "        [ 0.0844,  0.3014],\n",
      "        [-0.2559,  0.1449],\n",
      "        [-0.6552, -0.1483],\n",
      "        [ 0.0642,  0.5240],\n",
      "        [ 0.1475, -0.3458],\n",
      "        [ 0.1353, -0.5096],\n",
      "        [ 0.0852, -0.5498],\n",
      "        [-0.0667, -0.2467],\n",
      "        [-0.1086,  0.2258],\n",
      "        [-0.1188,  0.0322],\n",
      "        [-0.2790,  0.1833],\n",
      "        [ 0.1510, -0.3886],\n",
      "        [ 0.1130, -0.5343],\n",
      "        [-0.0509, -0.0555],\n",
      "        [-0.2780,  0.2012],\n",
      "        [-0.4766,  0.3549],\n",
      "        [-0.1124,  0.1825],\n",
      "        [-0.0471, -0.0634],\n",
      "        [ 0.3565,  0.4128],\n",
      "        [ 0.1136, -0.2172],\n",
      "        [-0.2847,  0.6498],\n",
      "        [ 0.1698, -0.2765],\n",
      "        [-0.4433,  0.3423],\n",
      "        [ 0.2139, -0.0443],\n",
      "        [ 0.0071, -0.1561],\n",
      "        [ 0.0620,  0.5307],\n",
      "        [-0.0811,  0.0516],\n",
      "        [ 0.2106,  0.0169],\n",
      "        [ 0.1567, -0.3175],\n",
      "        [ 0.0916, -0.2958]], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "210 \t training  0.22389909332769145\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 0.1174,  0.6088],\n",
      "        [-0.1875,  0.3859],\n",
      "        [-0.5383, -0.4491],\n",
      "        [ 0.1585, -0.4750],\n",
      "        [ 0.1382,  0.5992],\n",
      "        [-0.0183,  0.1403],\n",
      "        [-0.1179,  0.2253],\n",
      "        [ 0.1126, -0.2064],\n",
      "        [ 0.1632, -0.5395],\n",
      "        [ 0.2012, -0.3988],\n",
      "        [-0.0987,  0.0312],\n",
      "        [ 0.1239,  0.2131],\n",
      "        [-0.1549,  0.4520],\n",
      "        [-0.7334, -0.2098],\n",
      "        [ 0.0275,  0.4355],\n",
      "        [ 0.0880, -0.5468],\n",
      "        [-0.2680,  0.1440],\n",
      "        [ 0.0469,  0.4520],\n",
      "        [ 0.4049,  0.0559],\n",
      "        [ 0.1491, -0.3180],\n",
      "        [ 0.0979, -0.5698],\n",
      "        [ 0.1182, -0.2180],\n",
      "        [-0.5383, -0.4491],\n",
      "        [ 0.1579, -0.3430],\n",
      "        [ 0.0277,  0.4626],\n",
      "        [ 0.1512, -0.5039],\n",
      "        [-0.0569,  0.0007],\n",
      "        [-0.0402,  0.2302],\n",
      "        [-0.1104, -0.1445],\n",
      "        [-0.1858,  0.3925],\n",
      "        [-0.0488,  0.7288],\n",
      "        [ 0.0443,  0.3646],\n",
      "        [-0.4700,  0.5020],\n",
      "        [ 0.1658,  0.1276],\n",
      "        [ 0.0720,  0.3051],\n",
      "        [-0.2520,  0.1253],\n",
      "        [-0.6518, -0.1323],\n",
      "        [ 0.0649,  0.5227],\n",
      "        [ 0.1597, -0.3538],\n",
      "        [ 0.1490, -0.5104],\n",
      "        [ 0.0911, -0.5466],\n",
      "        [-0.0756, -0.2433],\n",
      "        [-0.1179,  0.2253],\n",
      "        [-0.1275,  0.0331],\n",
      "        [-0.2746,  0.1655],\n",
      "        [ 0.1638, -0.3949],\n",
      "        [ 0.1054, -0.5332],\n",
      "        [-0.0398, -0.0650],\n",
      "        [-0.2733,  0.1844],\n",
      "        [-0.4792,  0.3307],\n",
      "        [-0.1216,  0.1823],\n",
      "        [-0.0360, -0.0728],\n",
      "        [ 0.3544,  0.3922],\n",
      "        [ 0.1234, -0.2297],\n",
      "        [-0.2920,  0.6217],\n",
      "        [ 0.1607, -0.2774],\n",
      "        [-0.4441,  0.3211],\n",
      "        [ 0.1990, -0.0446],\n",
      "        [ 0.0194, -0.1645],\n",
      "        [ 0.0612,  0.5299],\n",
      "        [-0.0709,  0.0429],\n",
      "        [ 0.1951,  0.0174],\n",
      "        [ 0.1491, -0.3180],\n",
      "        [ 0.1059, -0.3003]], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "220 \t training  0.21771011736961335\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 0.1140,  0.5895],\n",
      "        [-0.1918,  0.3924],\n",
      "        [-0.5320, -0.4159],\n",
      "        [ 0.1702, -0.4764],\n",
      "        [ 0.1350,  0.5801],\n",
      "        [-0.0269,  0.1547],\n",
      "        [-0.1262,  0.2253],\n",
      "        [ 0.1209, -0.2168],\n",
      "        [ 0.1671, -0.5023],\n",
      "        [ 0.2082, -0.4016],\n",
      "        [-0.0941,  0.0154],\n",
      "        [ 0.1118,  0.2153],\n",
      "        [-0.1575,  0.4557],\n",
      "        [-0.7291, -0.1891],\n",
      "        [ 0.0204,  0.4367],\n",
      "        [ 0.0945, -0.5458],\n",
      "        [-0.2643,  0.1283],\n",
      "        [ 0.0495,  0.4490],\n",
      "        [ 0.4075,  0.0436],\n",
      "        [ 0.1430, -0.3185],\n",
      "        [ 0.1060, -0.5664],\n",
      "        [ 0.1266, -0.2281],\n",
      "        [-0.5320, -0.4159],\n",
      "        [ 0.1683, -0.3494],\n",
      "        [ 0.0215,  0.4633],\n",
      "        [ 0.1628, -0.5041],\n",
      "        [-0.0519, -0.0145],\n",
      "        [-0.0337,  0.2250],\n",
      "        [-0.1181, -0.1423],\n",
      "        [-0.1899,  0.3988],\n",
      "        [-0.0592,  0.7265],\n",
      "        [ 0.0352,  0.3667],\n",
      "        [-0.4753,  0.4767],\n",
      "        [ 0.1529,  0.1287],\n",
      "        [ 0.0615,  0.3075],\n",
      "        [-0.2484,  0.1090],\n",
      "        [-0.6474, -0.1183],\n",
      "        [ 0.0645,  0.5207],\n",
      "        [ 0.1702, -0.3598],\n",
      "        [ 0.1606, -0.5104],\n",
      "        [ 0.0963, -0.5436],\n",
      "        [-0.0836, -0.2406],\n",
      "        [-0.1262,  0.2253],\n",
      "        [-0.1353,  0.0342],\n",
      "        [-0.2708,  0.1507],\n",
      "        [ 0.1748, -0.3995],\n",
      "        [ 0.0991, -0.5323],\n",
      "        [-0.0308, -0.0728],\n",
      "        [-0.2694,  0.1704],\n",
      "        [-0.4811,  0.3103],\n",
      "        [-0.1297,  0.1825],\n",
      "        [-0.0269, -0.0806],\n",
      "        [ 0.3533,  0.3757],\n",
      "        [ 0.1321, -0.2394],\n",
      "        [-0.2969,  0.5985],\n",
      "        [ 0.1532, -0.2783],\n",
      "        [-0.4447,  0.3032],\n",
      "        [ 0.1865, -0.0455],\n",
      "        [ 0.0295, -0.1713],\n",
      "        [ 0.0597,  0.5283],\n",
      "        [-0.0628,  0.0355],\n",
      "        [ 0.1821,  0.0171],\n",
      "        [ 0.1430, -0.3185],\n",
      "        [ 0.1178, -0.3036]], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "230 \t training  0.21260662963063123\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 0.1124,  0.5739],\n",
      "        [-0.1951,  0.3976],\n",
      "        [-0.5251, -0.3871],\n",
      "        [ 0.1800, -0.4771],\n",
      "        [ 0.1334,  0.5647],\n",
      "        [-0.0338,  0.1666],\n",
      "        [-0.1338,  0.2260],\n",
      "        [ 0.1283, -0.2250],\n",
      "        [ 0.1711, -0.4701],\n",
      "        [ 0.2143, -0.4029],\n",
      "        [-0.0896,  0.0025],\n",
      "        [ 0.1016,  0.2164],\n",
      "        [-0.1595,  0.4584],\n",
      "        [-0.7238, -0.1710],\n",
      "        [ 0.0141,  0.4370],\n",
      "        [ 0.1003, -0.5440],\n",
      "        [-0.2609,  0.1153],\n",
      "        [ 0.0506,  0.4453],\n",
      "        [ 0.4102,  0.0342],\n",
      "        [ 0.1379, -0.3191],\n",
      "        [ 0.1128, -0.5632],\n",
      "        [ 0.1342, -0.2360],\n",
      "        [-0.5251, -0.3871],\n",
      "        [ 0.1772, -0.3541],\n",
      "        [ 0.0158,  0.4632],\n",
      "        [ 0.1727, -0.5037],\n",
      "        [-0.0470, -0.0269],\n",
      "        [-0.0292,  0.2200],\n",
      "        [-0.1251, -0.1402],\n",
      "        [-0.1930,  0.4038],\n",
      "        [-0.0687,  0.7253],\n",
      "        [ 0.0274,  0.3679],\n",
      "        [-0.4785,  0.4556],\n",
      "        [ 0.1419,  0.1291],\n",
      "        [ 0.0526,  0.3089],\n",
      "        [-0.2450,  0.0956],\n",
      "        [-0.6420, -0.1058],\n",
      "        [ 0.0632,  0.5178],\n",
      "        [ 0.1792, -0.3642],\n",
      "        [ 0.1704, -0.5097],\n",
      "        [ 0.1007, -0.5407],\n",
      "        [-0.0910, -0.2380],\n",
      "        [-0.1338,  0.2260],\n",
      "        [-0.1424,  0.0356],\n",
      "        [-0.2674,  0.1383],\n",
      "        [ 0.1842, -0.4027],\n",
      "        [ 0.0939, -0.5314],\n",
      "        [-0.0235, -0.0792],\n",
      "        [-0.2660,  0.1586],\n",
      "        [-0.4823,  0.2930],\n",
      "        [-0.1372,  0.1833],\n",
      "        [-0.0195, -0.0870],\n",
      "        [ 0.3532,  0.3626],\n",
      "        [ 0.1397, -0.2471],\n",
      "        [-0.2995,  0.5794],\n",
      "        [ 0.1469, -0.2792],\n",
      "        [-0.4450,  0.2881],\n",
      "        [ 0.1759, -0.0467],\n",
      "        [ 0.0379, -0.1768],\n",
      "        [ 0.0576,  0.5259],\n",
      "        [-0.0564,  0.0292],\n",
      "        [ 0.1710,  0.0164],\n",
      "        [ 0.1379, -0.3191],\n",
      "        [ 0.1274, -0.3062]], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "240 \t training  0.20825557544530746\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 0.1125,  0.5613],\n",
      "        [-0.1973,  0.4018],\n",
      "        [-0.5176, -0.3621],\n",
      "        [ 0.1883, -0.4770],\n",
      "        [ 0.1336,  0.5522],\n",
      "        [-0.0391,  0.1765],\n",
      "        [-0.1407,  0.2273],\n",
      "        [ 0.1351, -0.2312],\n",
      "        [ 0.1752, -0.4424],\n",
      "        [ 0.2198, -0.4030],\n",
      "        [-0.0849, -0.0080],\n",
      "        [ 0.0928,  0.2169],\n",
      "        [-0.1609,  0.4603],\n",
      "        [-0.7173, -0.1551],\n",
      "        [ 0.0084,  0.4366],\n",
      "        [ 0.1055, -0.5416],\n",
      "        [-0.2576,  0.1044],\n",
      "        [ 0.0502,  0.4408],\n",
      "        [ 0.4132,  0.0271],\n",
      "        [ 0.1334, -0.3198],\n",
      "        [ 0.1184, -0.5602],\n",
      "        [ 0.1411, -0.2421],\n",
      "        [-0.5176, -0.3621],\n",
      "        [ 0.1850, -0.3575],\n",
      "        [ 0.0107,  0.4623],\n",
      "        [ 0.1809, -0.5029],\n",
      "        [-0.0420, -0.0368],\n",
      "        [-0.0264,  0.2149],\n",
      "        [-0.1316, -0.1378],\n",
      "        [-0.1952,  0.4077],\n",
      "        [-0.0775,  0.7252],\n",
      "        [ 0.0206,  0.3682],\n",
      "        [-0.4797,  0.4379],\n",
      "        [ 0.1325,  0.1289],\n",
      "        [ 0.0449,  0.3095],\n",
      "        [-0.2415,  0.0844],\n",
      "        [-0.6356, -0.0947],\n",
      "        [ 0.0610,  0.5141],\n",
      "        [ 0.1871, -0.3673],\n",
      "        [ 0.1786, -0.5087],\n",
      "        [ 0.1043, -0.5380],\n",
      "        [-0.0979, -0.2355],\n",
      "        [-0.1407,  0.2273],\n",
      "        [-0.1489,  0.0376],\n",
      "        [-0.2643,  0.1279],\n",
      "        [ 0.1923, -0.4048],\n",
      "        [ 0.0894, -0.5305],\n",
      "        [-0.0176, -0.0847],\n",
      "        [-0.2630,  0.1486],\n",
      "        [-0.4828,  0.2783],\n",
      "        [-0.1440,  0.1848],\n",
      "        [-0.0135, -0.0925],\n",
      "        [ 0.3542,  0.3522],\n",
      "        [ 0.1467, -0.2529],\n",
      "        [-0.2999,  0.5635],\n",
      "        [ 0.1414, -0.2803],\n",
      "        [-0.4448,  0.2750],\n",
      "        [ 0.1667, -0.0482],\n",
      "        [ 0.0447, -0.1814],\n",
      "        [ 0.0548,  0.5226],\n",
      "        [-0.0516,  0.0235],\n",
      "        [ 0.1615,  0.0152],\n",
      "        [ 0.1334, -0.3198],\n",
      "        [ 0.1352, -0.3083]], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "250 \t training  0.20441726748748\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 0.1144,  0.5512],\n",
      "        [-0.1987,  0.4051],\n",
      "        [-0.5094, -0.3404],\n",
      "        [ 0.1953, -0.4765],\n",
      "        [ 0.1355,  0.5422],\n",
      "        [-0.0431,  0.1849],\n",
      "        [-0.1471,  0.2295],\n",
      "        [ 0.1414, -0.2360],\n",
      "        [ 0.1794, -0.4184],\n",
      "        [ 0.2248, -0.4022],\n",
      "        [-0.0801, -0.0164],\n",
      "        [ 0.0852,  0.2166],\n",
      "        [-0.1618,  0.4614],\n",
      "        [-0.7097, -0.1411],\n",
      "        [ 0.0033,  0.4353],\n",
      "        [ 0.1101, -0.5385],\n",
      "        [-0.2544,  0.0952],\n",
      "        [ 0.0486,  0.4355],\n",
      "        [ 0.4165,  0.0219],\n",
      "        [ 0.1292, -0.3207],\n",
      "        [ 0.1228, -0.5574],\n",
      "        [ 0.1474, -0.2466],\n",
      "        [-0.5094, -0.3404],\n",
      "        [ 0.1919, -0.3597],\n",
      "        [ 0.0059,  0.4606],\n",
      "        [ 0.1878, -0.5017],\n",
      "        [-0.0369, -0.0449],\n",
      "        [-0.0251,  0.2095],\n",
      "        [-0.1376, -0.1351],\n",
      "        [-0.1966,  0.4108],\n",
      "        [-0.0856,  0.7263],\n",
      "        [ 0.0146,  0.3678],\n",
      "        [-0.4791,  0.4231],\n",
      "        [ 0.1242,  0.1282],\n",
      "        [ 0.0383,  0.3094],\n",
      "        [-0.2380,  0.0751],\n",
      "        [-0.6281, -0.0847],\n",
      "        [ 0.0581,  0.5096],\n",
      "        [ 0.1940, -0.3693],\n",
      "        [ 0.1855, -0.5074],\n",
      "        [ 0.1071, -0.5357],\n",
      "        [-0.1043, -0.2329],\n",
      "        [-0.1471,  0.2295],\n",
      "        [-0.1549,  0.0401],\n",
      "        [-0.2614,  0.1189],\n",
      "        [ 0.1993, -0.4060],\n",
      "        [ 0.0854, -0.5295],\n",
      "        [-0.0129, -0.0896],\n",
      "        [-0.2603,  0.1399],\n",
      "        [-0.4825,  0.2657],\n",
      "        [-0.1503,  0.1871],\n",
      "        [-0.0087, -0.0973],\n",
      "        [ 0.3562,  0.3442],\n",
      "        [ 0.1531, -0.2572],\n",
      "        [-0.2982,  0.5503],\n",
      "        [ 0.1364, -0.2816],\n",
      "        [-0.4441,  0.2637],\n",
      "        [ 0.1586, -0.0500],\n",
      "        [ 0.0502, -0.1853],\n",
      "        [ 0.0515,  0.5185],\n",
      "        [-0.0480,  0.0183],\n",
      "        [ 0.1531,  0.0137],\n",
      "        [ 0.1292, -0.3207],\n",
      "        [ 0.1413, -0.3100]], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "260 \t training  0.2009198535499436\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 0.1178,  0.5430],\n",
      "        [-0.1993,  0.4076],\n",
      "        [-0.5005, -0.3215],\n",
      "        [ 0.2012, -0.4756],\n",
      "        [ 0.1389,  0.5342],\n",
      "        [-0.0458,  0.1920],\n",
      "        [-0.1531,  0.2325],\n",
      "        [ 0.1474, -0.2395],\n",
      "        [ 0.1836, -0.3976],\n",
      "        [ 0.2293, -0.4004],\n",
      "        [-0.0750, -0.0233],\n",
      "        [ 0.0784,  0.2158],\n",
      "        [-0.1621,  0.4620],\n",
      "        [-0.7009, -0.1286],\n",
      "        [-0.0015,  0.4335],\n",
      "        [ 0.1142, -0.5350],\n",
      "        [-0.2512,  0.0873],\n",
      "        [ 0.0460,  0.4295],\n",
      "        [ 0.4202,  0.0184],\n",
      "        [ 0.1252, -0.3218],\n",
      "        [ 0.1263, -0.5549],\n",
      "        [ 0.1534, -0.2499],\n",
      "        [-0.5005, -0.3215],\n",
      "        [ 0.1980, -0.3611],\n",
      "        [ 0.0013,  0.4583],\n",
      "        [ 0.1935, -0.5002],\n",
      "        [-0.0316, -0.0513],\n",
      "        [-0.0250,  0.2038],\n",
      "        [-0.1432, -0.1319],\n",
      "        [-0.1971,  0.4132],\n",
      "        [-0.0931,  0.7284],\n",
      "        [ 0.0092,  0.3667],\n",
      "        [-0.4768,  0.4105],\n",
      "        [ 0.1169,  0.1269],\n",
      "        [ 0.0324,  0.3087],\n",
      "        [-0.2343,  0.0672],\n",
      "        [-0.6194, -0.0757],\n",
      "        [ 0.0547,  0.5043],\n",
      "        [ 0.2001, -0.3705],\n",
      "        [ 0.1911, -0.5058],\n",
      "        [ 0.1091, -0.5336],\n",
      "        [-0.1103, -0.2300],\n",
      "        [-0.1531,  0.2325],\n",
      "        [-0.1605,  0.0433],\n",
      "        [-0.2585,  0.1112],\n",
      "        [ 0.2054, -0.4064],\n",
      "        [ 0.0817, -0.5285],\n",
      "        [-0.0092, -0.0939],\n",
      "        [-0.2578,  0.1323],\n",
      "        [-0.4815,  0.2547],\n",
      "        [-0.1561,  0.1901],\n",
      "        [-0.0050, -0.1016],\n",
      "        [ 0.3594,  0.3379],\n",
      "        [ 0.1592, -0.2604],\n",
      "        [-0.2947,  0.5393],\n",
      "        [ 0.1317, -0.2830],\n",
      "        [-0.4429,  0.2538],\n",
      "        [ 0.1512, -0.0521],\n",
      "        [ 0.0546, -0.1887],\n",
      "        [ 0.0479,  0.5137],\n",
      "        [-0.0456,  0.0133],\n",
      "        [ 0.1455,  0.0118],\n",
      "        [ 0.1252, -0.3218],\n",
      "        [ 0.1460, -0.3115]], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "270 \t training  0.1976422007203498\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 0.1226,  0.5363],\n",
      "        [-0.1991,  0.4096],\n",
      "        [-0.4910, -0.3051],\n",
      "        [ 0.2062, -0.4743],\n",
      "        [ 0.1436,  0.5276],\n",
      "        [-0.0476,  0.1979],\n",
      "        [-0.1585,  0.2363],\n",
      "        [ 0.1531, -0.2419],\n",
      "        [ 0.1879, -0.3797],\n",
      "        [ 0.2336, -0.3979],\n",
      "        [-0.0698, -0.0288],\n",
      "        [ 0.0724,  0.2145],\n",
      "        [-0.1619,  0.4620],\n",
      "        [-0.6910, -0.1175],\n",
      "        [-0.0059,  0.4310],\n",
      "        [ 0.1180, -0.5310],\n",
      "        [-0.2479,  0.0805],\n",
      "        [ 0.0426,  0.4227],\n",
      "        [ 0.4242,  0.0162],\n",
      "        [ 0.1212, -0.3231],\n",
      "        [ 0.1289, -0.5526],\n",
      "        [ 0.1591, -0.2522],\n",
      "        [-0.4910, -0.3051],\n",
      "        [ 0.2035, -0.3617],\n",
      "        [-0.0030,  0.4553],\n",
      "        [ 0.1982, -0.4986],\n",
      "        [-0.0262, -0.0564],\n",
      "        [-0.0261,  0.1976],\n",
      "        [-0.1484, -0.1281],\n",
      "        [-0.1969,  0.4150],\n",
      "        [-0.1001,  0.7316],\n",
      "        [ 0.0044,  0.3651],\n",
      "        [-0.4730,  0.3997],\n",
      "        [ 0.1102,  0.1252],\n",
      "        [ 0.0271,  0.3074],\n",
      "        [-0.2306,  0.0604],\n",
      "        [-0.6095, -0.0674],\n",
      "        [ 0.0507,  0.4982],\n",
      "        [ 0.2056, -0.3710],\n",
      "        [ 0.1957, -0.5040],\n",
      "        [ 0.1104, -0.5319],\n",
      "        [-0.1159, -0.2266],\n",
      "        [-0.1585,  0.2363],\n",
      "        [-0.1656,  0.0473],\n",
      "        [-0.2558,  0.1043],\n",
      "        [ 0.2108, -0.4063],\n",
      "        [ 0.0784, -0.5275],\n",
      "        [-0.0065, -0.0980],\n",
      "        [-0.2554,  0.1254],\n",
      "        [-0.4798,  0.2450],\n",
      "        [-0.1615,  0.1940],\n",
      "        [-0.0022, -0.1056],\n",
      "        [ 0.3634,  0.3331],\n",
      "        [ 0.1648, -0.2625],\n",
      "        [-0.2895,  0.5300],\n",
      "        [ 0.1271, -0.2847],\n",
      "        [-0.4411,  0.2449],\n",
      "        [ 0.1444, -0.0545],\n",
      "        [ 0.0580, -0.1918],\n",
      "        [ 0.0438,  0.5081],\n",
      "        [-0.0441,  0.0083],\n",
      "        [ 0.1386,  0.0095],\n",
      "        [ 0.1212, -0.3231],\n",
      "        [ 0.1495, -0.3130]], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "280 \t training  0.19450192676302255\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 1.2857e-01,  5.3074e-01],\n",
      "        [-1.9809e-01,  4.1099e-01],\n",
      "        [-4.8107e-01, -2.9078e-01],\n",
      "        [ 2.1026e-01, -4.7284e-01],\n",
      "        [ 1.4946e-01,  5.2218e-01],\n",
      "        [-4.8543e-02,  2.0294e-01],\n",
      "        [-1.6358e-01,  2.4097e-01],\n",
      "        [ 1.5848e-01, -2.4362e-01],\n",
      "        [ 1.9206e-01, -3.6426e-01],\n",
      "        [ 2.3762e-01, -3.9468e-01],\n",
      "        [-6.4447e-02, -3.3410e-02],\n",
      "        [ 6.6860e-02,  2.1272e-01],\n",
      "        [-1.6097e-01,  4.6161e-01],\n",
      "        [-6.8005e-01, -1.0755e-01],\n",
      "        [-1.0105e-02,  4.2796e-01],\n",
      "        [ 1.2156e-01, -5.2652e-01],\n",
      "        [-2.4467e-01,  7.4417e-02],\n",
      "        [ 3.8636e-02,  4.1513e-01],\n",
      "        [ 4.2851e-01,  1.5023e-02],\n",
      "        [ 1.1727e-01, -3.2466e-01],\n",
      "        [ 1.3070e-01, -5.5049e-01],\n",
      "        [ 1.6449e-01, -2.5379e-01],\n",
      "        [-4.8107e-01, -2.9078e-01],\n",
      "        [ 2.0844e-01, -3.6177e-01],\n",
      "        [-7.1330e-03,  4.5181e-01],\n",
      "        [ 2.0201e-01, -4.9680e-01],\n",
      "        [-2.0721e-02, -6.0574e-02],\n",
      "        [-2.7950e-02,  1.9097e-01],\n",
      "        [-1.5313e-01, -1.2375e-01],\n",
      "        [-1.9590e-01,  4.1621e-01],\n",
      "        [-1.0668e-01,  7.3578e-01],\n",
      "        [-5.7558e-05,  3.6288e-01],\n",
      "        [-4.6797e-01,  3.9034e-01],\n",
      "        [ 1.0410e-01,  1.2307e-01],\n",
      "        [ 2.2246e-02,  3.0553e-01],\n",
      "        [-2.2680e-01,  5.4548e-02],\n",
      "        [-5.9841e-01, -5.9944e-02],\n",
      "        [ 4.6480e-02,  4.9151e-01],\n",
      "        [ 2.1046e-01, -3.7090e-01],\n",
      "        [ 1.9950e-01, -5.0219e-01],\n",
      "        [ 1.1108e-01, -5.3030e-01],\n",
      "        [-1.2100e-01, -2.2275e-01],\n",
      "        [-1.6358e-01,  2.4097e-01],\n",
      "        [-1.7028e-01,  5.1937e-02],\n",
      "        [-2.5316e-01,  9.8096e-02],\n",
      "        [ 2.1542e-01, -4.0581e-01],\n",
      "        [ 7.5281e-02, -5.2629e-01],\n",
      "        [-4.6241e-03, -1.0201e-01],\n",
      "        [-2.5325e-01,  1.1909e-01],\n",
      "        [-4.7736e-01,  2.3637e-01],\n",
      "        [-1.6645e-01,  1.9868e-01],\n",
      "        [-2.3911e-04, -1.0947e-01],\n",
      "        [ 3.6825e-01,  3.2946e-01],\n",
      "        [ 1.7016e-01, -2.6395e-01],\n",
      "        [-2.8302e-01,  5.2198e-01],\n",
      "        [ 1.2261e-01, -2.8655e-01],\n",
      "        [-4.3874e-01,  2.3689e-01],\n",
      "        [ 1.3802e-01, -5.7233e-02],\n",
      "        [ 6.0526e-02, -1.9466e-01],\n",
      "        [ 3.9607e-02,  5.0190e-01],\n",
      "        [-4.3505e-02,  3.2488e-03],\n",
      "        [ 1.3215e-01,  6.9631e-03],\n",
      "        [ 1.1727e-01, -3.2466e-01],\n",
      "        [ 1.5188e-01, -3.1442e-01]], dtype=torch.float64,\n",
      "       grad_fn=<SubBackward0>)\n",
      "290 \t training  0.1914459873122841\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 0.1353,  0.5260],\n",
      "        [-0.1965,  0.4119],\n",
      "        [-0.4708, -0.2784],\n",
      "        [ 0.2136, -0.4712],\n",
      "        [ 0.1561,  0.5175],\n",
      "        [-0.0488,  0.2071],\n",
      "        [-0.1682,  0.2464],\n",
      "        [ 0.1635, -0.2447],\n",
      "        [ 0.1960, -0.3509],\n",
      "        [ 0.2414, -0.3910],\n",
      "        [-0.0591, -0.0372],\n",
      "        [ 0.0618,  0.2105],\n",
      "        [-0.1595,  0.4608],\n",
      "        [-0.6682, -0.0987],\n",
      "        [-0.0140,  0.4245],\n",
      "        [ 0.1248, -0.5217],\n",
      "        [-0.2414,  0.0689],\n",
      "        [ 0.0342,  0.4070],\n",
      "        [ 0.4329,  0.0146],\n",
      "        [ 0.1135, -0.3263],\n",
      "        [ 0.1320, -0.5485],\n",
      "        [ 0.1695, -0.2548],\n",
      "        [-0.4708, -0.2784],\n",
      "        [ 0.2128, -0.3614],\n",
      "        [-0.0110,  0.4478],\n",
      "        [ 0.2051, -0.4949],\n",
      "        [-0.0152, -0.0640],\n",
      "        [-0.0305,  0.1839],\n",
      "        [-0.1574, -0.1188],\n",
      "        [-0.1943,  0.4170],\n",
      "        [-0.1129,  0.7408],\n",
      "        [-0.0041,  0.3603],\n",
      "        [-0.4620,  0.3821],\n",
      "        [ 0.0985,  0.1206],\n",
      "        [ 0.0179,  0.3033],\n",
      "        [-0.2230,  0.0493],\n",
      "        [-0.5863, -0.0531],\n",
      "        [ 0.0420,  0.4842],\n",
      "        [ 0.2148, -0.3705],\n",
      "        [ 0.2025, -0.5002],\n",
      "        [ 0.1114, -0.5289],\n",
      "        [-0.1257, -0.2184],\n",
      "        [-0.1682,  0.2464],\n",
      "        [-0.1746,  0.0573],\n",
      "        [-0.2506,  0.0924],\n",
      "        [ 0.2194, -0.4049],\n",
      "        [ 0.0725, -0.5249],\n",
      "        [-0.0034, -0.1059],\n",
      "        [-0.2511,  0.1132],\n",
      "        [-0.4742,  0.2286],\n",
      "        [-0.1710,  0.2041],\n",
      "        [ 0.0010, -0.1133],\n",
      "        [ 0.3735,  0.3266],\n",
      "        [ 0.1751, -0.2648],\n",
      "        [-0.2755,  0.5149],\n",
      "        [ 0.1183, -0.2885],\n",
      "        [-0.4357,  0.2296],\n",
      "        [ 0.1321, -0.0601],\n",
      "        [ 0.0623, -0.1975],\n",
      "        [ 0.0353,  0.4952],\n",
      "        [-0.0436, -0.0019],\n",
      "        [ 0.1262,  0.0041],\n",
      "        [ 0.1135, -0.3263],\n",
      "        [ 0.1534, -0.3159]], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "300 \t training  0.18844216424321045\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 0.1425,  0.5217],\n",
      "        [-0.1943,  0.4124],\n",
      "        [-0.4605, -0.2677],\n",
      "        [ 0.2164, -0.4694],\n",
      "        [ 0.1632,  0.5134],\n",
      "        [-0.0485,  0.2106],\n",
      "        [-0.1726,  0.2525],\n",
      "        [ 0.1682, -0.2454],\n",
      "        [ 0.1995, -0.3394],\n",
      "        [ 0.2447, -0.3869],\n",
      "        [-0.0537, -0.0405],\n",
      "        [ 0.0572,  0.2080],\n",
      "        [-0.1575,  0.4596],\n",
      "        [-0.6556, -0.0909],\n",
      "        [-0.0176,  0.4206],\n",
      "        [ 0.1278, -0.5165],\n",
      "        [-0.2382,  0.0639],\n",
      "        [ 0.0294,  0.3984],\n",
      "        [ 0.4372,  0.0148],\n",
      "        [ 0.1098, -0.3279],\n",
      "        [ 0.1328, -0.5465],\n",
      "        [ 0.1741, -0.2553],\n",
      "        [-0.4605, -0.2677],\n",
      "        [ 0.2166, -0.3608],\n",
      "        [-0.0147,  0.4435],\n",
      "        [ 0.2075, -0.4929],\n",
      "        [-0.0098, -0.0668],\n",
      "        [-0.0336,  0.1766],\n",
      "        [-0.1613, -0.1132],\n",
      "        [-0.1921,  0.4174],\n",
      "        [-0.1189,  0.7466],\n",
      "        [-0.0078,  0.3572],\n",
      "        [-0.4552,  0.3746],\n",
      "        [ 0.0933,  0.1179],\n",
      "        [ 0.0139,  0.3006],\n",
      "        [-0.2192,  0.0446],\n",
      "        [-0.5733, -0.0471],\n",
      "        [ 0.0375,  0.4765],\n",
      "        [ 0.2185, -0.3697],\n",
      "        [ 0.2049, -0.4982],\n",
      "        [ 0.1114, -0.5274],\n",
      "        [-0.1299, -0.2134],\n",
      "        [-0.1726,  0.2525],\n",
      "        [-0.1785,  0.0633],\n",
      "        [-0.2480,  0.0870],\n",
      "        [ 0.2228, -0.4038],\n",
      "        [ 0.0701, -0.5233],\n",
      "        [-0.0028, -0.1098],\n",
      "        [-0.2491,  0.1076],\n",
      "        [-0.4704,  0.2215],\n",
      "        [-0.1753,  0.2102],\n",
      "        [ 0.0017, -0.1171],\n",
      "        [ 0.3790,  0.3242],\n",
      "        [ 0.1797, -0.2652],\n",
      "        [-0.2673,  0.5084],\n",
      "        [ 0.1142, -0.2905],\n",
      "        [-0.4320,  0.2228],\n",
      "        [ 0.1265, -0.0631],\n",
      "        [ 0.0634, -0.2002],\n",
      "        [ 0.0309,  0.4879],\n",
      "        [-0.0442, -0.0071],\n",
      "        [ 0.1206,  0.0012],\n",
      "        [ 0.1098, -0.3279],\n",
      "        [ 0.1540, -0.3174]], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "310 \t training  0.18547099146550602\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 0.1499,  0.5178],\n",
      "        [-0.1917,  0.4125],\n",
      "        [-0.4502, -0.2585],\n",
      "        [ 0.2185, -0.4676],\n",
      "        [ 0.1704,  0.5096],\n",
      "        [-0.0478,  0.2133],\n",
      "        [-0.1768,  0.2591],\n",
      "        [ 0.1724, -0.2457],\n",
      "        [ 0.2025, -0.3295],\n",
      "        [ 0.2476, -0.3826],\n",
      "        [-0.0486, -0.0433],\n",
      "        [ 0.0531,  0.2052],\n",
      "        [-0.1552,  0.4581],\n",
      "        [-0.6427, -0.0841],\n",
      "        [-0.0210,  0.4163],\n",
      "        [ 0.1305, -0.5112],\n",
      "        [-0.2350,  0.0592],\n",
      "        [ 0.0245,  0.3895],\n",
      "        [ 0.4411,  0.0152],\n",
      "        [ 0.1065, -0.3293],\n",
      "        [ 0.1333, -0.5444],\n",
      "        [ 0.1783, -0.2556],\n",
      "        [-0.4502, -0.2585],\n",
      "        [ 0.2198, -0.3600],\n",
      "        [-0.0182,  0.4387],\n",
      "        [ 0.2094, -0.4909],\n",
      "        [-0.0047, -0.0693],\n",
      "        [-0.0370,  0.1690],\n",
      "        [-0.1649, -0.1071],\n",
      "        [-0.1895,  0.4173],\n",
      "        [-0.1249,  0.7529],\n",
      "        [-0.0112,  0.3539],\n",
      "        [-0.4478,  0.3677],\n",
      "        [ 0.0886,  0.1150],\n",
      "        [ 0.0103,  0.2977],\n",
      "        [-0.2154,  0.0402],\n",
      "        [-0.5597, -0.0417],\n",
      "        [ 0.0329,  0.4684],\n",
      "        [ 0.2216, -0.3688],\n",
      "        [ 0.2067, -0.4961],\n",
      "        [ 0.1112, -0.5259],\n",
      "        [-0.1337, -0.2079],\n",
      "        [-0.1768,  0.2591],\n",
      "        [-0.1822,  0.0699],\n",
      "        [-0.2454,  0.0820],\n",
      "        [ 0.2257, -0.4026],\n",
      "        [ 0.0682, -0.5213],\n",
      "        [-0.0027, -0.1136],\n",
      "        [-0.2470,  0.1022],\n",
      "        [-0.4658,  0.2151],\n",
      "        [-0.1794,  0.2169],\n",
      "        [ 0.0019, -0.1208],\n",
      "        [ 0.3844,  0.3222],\n",
      "        [ 0.1837, -0.2654],\n",
      "        [-0.2587,  0.5024],\n",
      "        [ 0.1105, -0.2923],\n",
      "        [-0.4276,  0.2167],\n",
      "        [ 0.1215, -0.0661],\n",
      "        [ 0.0640, -0.2030],\n",
      "        [ 0.0265,  0.4804],\n",
      "        [-0.0452, -0.0123],\n",
      "        [ 0.1156, -0.0019],\n",
      "        [ 0.1065, -0.3293],\n",
      "        [ 0.1540, -0.3190]], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "320 \t training  0.18251865954437865\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 1.5708e-01,  5.1399e-01],\n",
      "        [-1.8876e-01,  4.1208e-01],\n",
      "        [-4.4041e-01, -2.5069e-01],\n",
      "        [ 2.2003e-01, -4.6562e-01],\n",
      "        [ 1.7745e-01,  5.0591e-01],\n",
      "        [-4.6853e-02,  2.1547e-01],\n",
      "        [-1.8100e-01,  2.6625e-01],\n",
      "        [ 1.7614e-01, -2.4590e-01],\n",
      "        [ 2.0479e-01, -3.2098e-01],\n",
      "        [ 2.4992e-01, -3.7813e-01],\n",
      "        [-4.3692e-02, -4.5886e-02],\n",
      "        [ 4.9347e-02,  2.0228e-01],\n",
      "        [-1.5253e-01,  4.5613e-01],\n",
      "        [-6.2949e-01, -7.8331e-02],\n",
      "        [-2.4203e-02,  4.1180e-01],\n",
      "        [ 1.3276e-01, -5.0560e-01],\n",
      "        [-2.3169e-01,  5.4812e-02],\n",
      "        [ 1.9611e-02,  3.8037e-01],\n",
      "        [ 4.4439e-01,  1.5926e-02],\n",
      "        [ 1.0372e-01, -3.3040e-01],\n",
      "        [ 1.3368e-01, -5.4213e-01],\n",
      "        [ 1.8187e-01, -2.5562e-01],\n",
      "        [-4.4041e-01, -2.5069e-01],\n",
      "        [ 2.2246e-01, -3.5898e-01],\n",
      "        [-2.1478e-02,  4.3367e-01],\n",
      "        [ 2.1070e-01, -4.8873e-01],\n",
      "        [ 1.8971e-04, -7.1512e-02],\n",
      "        [-4.0571e-02,  1.6135e-01],\n",
      "        [-1.6814e-01, -1.0036e-01],\n",
      "        [-1.8660e-01,  4.1674e-01],\n",
      "        [-1.3093e-01,  7.5962e-01],\n",
      "        [-1.4352e-02,  3.5026e-01],\n",
      "        [-4.3999e-01,  3.6122e-01],\n",
      "        [ 8.4442e-02,  1.1203e-01],\n",
      "        [ 7.0189e-03,  2.9450e-01],\n",
      "        [-2.1157e-01,  3.6163e-02],\n",
      "        [-5.4577e-01, -3.7114e-02],\n",
      "        [ 2.8305e-02,  4.5996e-01],\n",
      "        [ 2.2416e-01, -3.6774e-01],\n",
      "        [ 2.0793e-01, -4.9394e-01],\n",
      "        [ 1.1108e-01, -5.2410e-01],\n",
      "        [-1.3714e-01, -2.0182e-01],\n",
      "        [-1.8100e-01,  2.6625e-01],\n",
      "        [-1.8576e-01,  7.6991e-02],\n",
      "        [-2.4269e-01,  7.7217e-02],\n",
      "        [ 2.2786e-01, -4.0120e-01],\n",
      "        [ 6.6708e-02, -5.1885e-01],\n",
      "        [-2.9031e-03, -1.1743e-01],\n",
      "        [-2.4478e-01,  9.7203e-02],\n",
      "        [-4.6044e-01,  2.0936e-01],\n",
      "        [-1.8338e-01,  2.2399e-01],\n",
      "        [ 1.7316e-03, -1.2452e-01],\n",
      "        [ 3.8945e-01,  3.2030e-01],\n",
      "        [ 1.8726e-01, -2.6534e-01],\n",
      "        [-2.4988e-01,  4.9658e-01],\n",
      "        [ 1.0736e-01, -2.9375e-01],\n",
      "        [-4.2248e-01,  2.1123e-01],\n",
      "        [ 1.1705e-01, -6.8984e-02],\n",
      "        [ 6.4085e-02, -2.0564e-01],\n",
      "        [ 2.2217e-02,  4.7252e-01],\n",
      "        [-4.6559e-02, -1.7552e-02],\n",
      "        [ 1.1107e-01, -4.8430e-03],\n",
      "        [ 1.0372e-01, -3.3040e-01],\n",
      "        [ 1.5347e-01, -3.2051e-01]], dtype=torch.float64,\n",
      "       grad_fn=<SubBackward0>)\n",
      "330 \t training  0.17957178558069953\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 0.1639,  0.5102],\n",
      "        [-0.1857,  0.4112],\n",
      "        [-0.4312, -0.2441],\n",
      "        [ 0.2210, -0.4636],\n",
      "        [ 0.1841,  0.5023],\n",
      "        [-0.0458,  0.2170],\n",
      "        [-0.1853,  0.2738],\n",
      "        [ 0.1792, -0.2459],\n",
      "        [ 0.2063, -0.3137],\n",
      "        [ 0.2514, -0.3736],\n",
      "        [-0.0391, -0.0482],\n",
      "        [ 0.0460,  0.1992],\n",
      "        [-0.1497,  0.4538],\n",
      "        [-0.6164, -0.0735],\n",
      "        [-0.0272,  0.4069],\n",
      "        [ 0.1345, -0.4999],\n",
      "        [-0.2284,  0.0507],\n",
      "        [ 0.0147,  0.3711],\n",
      "        [ 0.4469,  0.0167],\n",
      "        [ 0.1015, -0.3310],\n",
      "        [ 0.1340, -0.5396],\n",
      "        [ 0.1849, -0.2556],\n",
      "        [-0.4312, -0.2441],\n",
      "        [ 0.2245, -0.3579],\n",
      "        [-0.0246,  0.4283],\n",
      "        [ 0.2115, -0.4865],\n",
      "        [ 0.0047, -0.0735],\n",
      "        [-0.0442,  0.1537],\n",
      "        [-0.1712, -0.0931],\n",
      "        [-0.1836,  0.4157],\n",
      "        [-0.1372,  0.7667],\n",
      "        [-0.0172,  0.3463],\n",
      "        [-0.4318,  0.3552],\n",
      "        [ 0.0808,  0.1090],\n",
      "        [ 0.0041,  0.2911],\n",
      "        [-0.2078,  0.0324],\n",
      "        [-0.5317, -0.0334],\n",
      "        [ 0.0238,  0.4513],\n",
      "        [ 0.2261, -0.3666],\n",
      "        [ 0.2087, -0.4917],\n",
      "        [ 0.1111, -0.5219],\n",
      "        [-0.1401, -0.1950],\n",
      "        [-0.1853,  0.2738],\n",
      "        [-0.1893,  0.0846],\n",
      "        [-0.2399,  0.0728],\n",
      "        [ 0.2294, -0.3997],\n",
      "        [ 0.0658, -0.5157],\n",
      "        [-0.0034, -0.1211],\n",
      "        [-0.2424,  0.0925],\n",
      "        [-0.4543,  0.2043],\n",
      "        [-0.1875,  0.2315],\n",
      "        [ 0.0013, -0.1281],\n",
      "        [ 0.3938,  0.3185],\n",
      "        [ 0.1902, -0.2652],\n",
      "        [-0.2411,  0.4910],\n",
      "        [ 0.1048, -0.2947],\n",
      "        [-0.4165,  0.2063],\n",
      "        [ 0.1132, -0.0716],\n",
      "        [ 0.0638, -0.2082],\n",
      "        [ 0.0180,  0.4644],\n",
      "        [-0.0480, -0.0227],\n",
      "        [ 0.1072, -0.0077],\n",
      "        [ 0.1015, -0.3310],\n",
      "        [ 0.1525, -0.3220]], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "340 \t training  0.17661459546360617\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 1.7002e-01,  5.0650e-01],\n",
      "        [-1.8270e-01,  4.0968e-01],\n",
      "        [-4.2280e-01, -2.3868e-01],\n",
      "        [ 2.2151e-01, -4.6148e-01],\n",
      "        [ 1.9002e-01,  4.9866e-01],\n",
      "        [-4.4694e-02,  2.1781e-01],\n",
      "        [-1.8985e-01,  2.8166e-01],\n",
      "        [ 1.8172e-01, -2.4587e-01],\n",
      "        [ 2.0683e-01, -3.0745e-01],\n",
      "        [ 2.5206e-01, -3.6914e-01],\n",
      "        [-3.4837e-02, -5.0364e-02],\n",
      "        [ 4.3172e-02,  1.9598e-01],\n",
      "        [-1.4688e-01,  4.5089e-01],\n",
      "        [-6.0363e-01, -6.9780e-02],\n",
      "        [-3.0112e-02,  4.0173e-01],\n",
      "        [ 1.3559e-01, -4.9415e-01],\n",
      "        [-2.2495e-01,  4.6968e-02],\n",
      "        [ 9.9052e-03,  3.6170e-01],\n",
      "        [ 4.4832e-01,  1.7476e-02],\n",
      "        [ 9.9992e-02, -3.3095e-01],\n",
      "        [ 1.3429e-01, -5.3671e-01],\n",
      "        [ 1.8726e-01, -2.5540e-01],\n",
      "        [-4.2280e-01, -2.3868e-01],\n",
      "        [ 2.2582e-01, -3.5678e-01],\n",
      "        [-2.7719e-02,  4.2259e-01],\n",
      "        [ 2.1187e-01, -4.8416e-01],\n",
      "        [ 8.7873e-03, -7.5306e-02],\n",
      "        [-4.7696e-02,  1.4615e-01],\n",
      "        [-1.7405e-01, -8.5136e-02],\n",
      "        [-1.8055e-01,  4.1409e-01],\n",
      "        [-1.4397e-01,  7.7402e-01],\n",
      "        [-1.9853e-02,  3.4212e-01],\n",
      "        [-4.2340e-01,  3.4945e-01],\n",
      "        [ 7.7639e-02,  1.0610e-01],\n",
      "        [ 1.4401e-03,  2.8737e-01],\n",
      "        [-2.0398e-01,  2.8965e-02],\n",
      "        [-5.1757e-01, -3.0473e-02],\n",
      "        [ 1.9315e-02,  4.4248e-01],\n",
      "        [ 2.2735e-01, -3.6538e-01],\n",
      "        [ 2.0904e-01, -4.8927e-01],\n",
      "        [ 1.1128e-01, -5.1932e-01],\n",
      "        [-1.4281e-01, -1.8750e-01],\n",
      "        [-1.8985e-01,  2.8166e-01],\n",
      "        [-1.9292e-01,  9.2610e-02],\n",
      "        [-2.3697e-01,  6.8644e-02],\n",
      "        [ 2.3041e-01, -3.9823e-01],\n",
      "        [ 6.5580e-02, -5.1184e-01],\n",
      "        [-4.1360e-03, -1.2470e-01],\n",
      "        [-2.3989e-01,  8.8017e-02],\n",
      "        [-4.4737e-01,  1.9981e-01],\n",
      "        [-1.9183e-01,  2.3948e-01],\n",
      "        [ 5.7693e-04, -1.3160e-01],\n",
      "        [ 3.9729e-01,  3.1672e-01],\n",
      "        [ 1.9246e-01, -2.6492e-01],\n",
      "        [-2.3248e-01,  4.8547e-01],\n",
      "        [ 1.0299e-01, -2.9511e-01],\n",
      "        [-4.0970e-01,  2.0205e-01],\n",
      "        [ 1.1015e-01, -7.3827e-02],\n",
      "        [ 6.3185e-02, -2.1070e-01],\n",
      "        [ 1.3727e-02,  4.5601e-01],\n",
      "        [-4.9595e-02, -2.7705e-02],\n",
      "        [ 1.0398e-01, -1.0208e-02],\n",
      "        [ 9.9992e-02, -3.3095e-01],\n",
      "        [ 1.5106e-01, -3.2338e-01]], dtype=torch.float64,\n",
      "       grad_fn=<SubBackward0>)\n",
      "350 \t training  0.17362846551101438\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 1.7540e-01,  5.0280e-01],\n",
      "        [-1.7976e-01,  4.0756e-01],\n",
      "        [-4.1542e-01, -2.3436e-01],\n",
      "        [ 2.2146e-01, -4.5928e-01],\n",
      "        [ 1.9520e-01,  4.9509e-01],\n",
      "        [-4.3682e-02,  2.1798e-01],\n",
      "        [-1.9486e-01,  2.8984e-01],\n",
      "        [ 1.8352e-01, -2.4572e-01],\n",
      "        [ 2.0637e-01, -3.0225e-01],\n",
      "        [ 2.5160e-01, -3.6476e-01],\n",
      "        [-3.0943e-02, -5.2312e-02],\n",
      "        [ 4.0748e-02,  1.9268e-01],\n",
      "        [-1.4407e-01,  4.4747e-01],\n",
      "        [-5.9132e-01, -6.7063e-02],\n",
      "        [-3.2918e-02,  3.9616e-01],\n",
      "        [ 1.3593e-01, -4.8835e-01],\n",
      "        [-2.2141e-01,  4.3508e-02],\n",
      "        [ 5.2022e-03,  3.5222e-01],\n",
      "        [ 4.4857e-01,  1.8249e-02],\n",
      "        [ 9.9330e-02, -3.3013e-01],\n",
      "        [ 1.3469e-01, -5.3339e-01],\n",
      "        [ 1.8896e-01, -2.5516e-01],\n",
      "        [-4.1542e-01, -2.3436e-01],\n",
      "        [ 2.2651e-01, -3.5560e-01],\n",
      "        [-3.0743e-02,  4.1649e-01],\n",
      "        [ 2.1177e-01, -4.8171e-01],\n",
      "        [ 1.2444e-02, -7.6942e-02],\n",
      "        [-5.1115e-02,  1.3868e-01],\n",
      "        [-1.7692e-01, -7.6617e-02],\n",
      "        [-1.7762e-01,  4.1185e-01],\n",
      "        [-1.5133e-01,  7.8154e-01],\n",
      "        [-2.2301e-02,  3.3757e-01],\n",
      "        [-4.1468e-01,  3.4413e-01],\n",
      "        [ 7.5109e-02,  1.0325e-01],\n",
      "        [-9.0837e-04,  2.8343e-01],\n",
      "        [-2.0014e-01,  2.5804e-02],\n",
      "        [-5.0365e-01, -2.8511e-02],\n",
      "        [ 1.4884e-02,  4.3338e-01],\n",
      "        [ 2.2796e-01, -3.6411e-01],\n",
      "        [ 2.0894e-01, -4.8676e-01],\n",
      "        [ 1.1181e-01, -5.1613e-01],\n",
      "        [-1.4522e-01, -1.7922e-01],\n",
      "        [-1.9486e-01,  2.8984e-01],\n",
      "        [-1.9682e-01,  1.0107e-01],\n",
      "        [-2.3382e-01,  6.4833e-02],\n",
      "        [ 2.3075e-01, -3.9666e-01],\n",
      "        [ 6.5998e-02, -5.0710e-01],\n",
      "        [-5.0172e-03, -1.2812e-01],\n",
      "        [-2.3709e-01,  8.3900e-02],\n",
      "        [-4.3951e-01,  1.9604e-01],\n",
      "        [-1.9660e-01,  2.4773e-01],\n",
      "        [-2.8028e-04, -1.3492e-01],\n",
      "        [ 3.9975e-01,  3.1497e-01],\n",
      "        [ 1.9406e-01, -2.6459e-01],\n",
      "        [-2.2406e-01,  4.8016e-01],\n",
      "        [ 1.0205e-01, -2.9473e-01],\n",
      "        [-4.0193e-01,  1.9843e-01],\n",
      "        [ 1.0789e-01, -7.5612e-02],\n",
      "        [ 6.2298e-02, -2.1305e-01],\n",
      "        [ 9.5178e-03,  4.4733e-01],\n",
      "        [-5.1179e-02, -3.2564e-02],\n",
      "        [ 1.0153e-01, -1.2432e-02],\n",
      "        [ 9.9330e-02, -3.3013e-01],\n",
      "        [ 1.4934e-01, -3.2463e-01]], dtype=torch.float64,\n",
      "       grad_fn=<SubBackward0>)\n",
      "360 \t training  0.17059333375192498\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 1.7995e-01,  4.9923e-01],\n",
      "        [-1.7701e-01,  4.0472e-01],\n",
      "        [-4.0916e-01, -2.3107e-01],\n",
      "        [ 2.2090e-01, -4.5697e-01],\n",
      "        [ 1.9951e-01,  4.9163e-01],\n",
      "        [-4.2801e-02,  2.1747e-01],\n",
      "        [-2.0052e-01,  2.9828e-01],\n",
      "        [ 1.8465e-01, -2.4546e-01],\n",
      "        [ 2.0485e-01, -2.9794e-01],\n",
      "        [ 2.4995e-01, -3.6051e-01],\n",
      "        [-2.7404e-02, -5.4029e-02],\n",
      "        [ 3.8795e-02,  1.8930e-01],\n",
      "        [-1.4137e-01,  4.4343e-01],\n",
      "        [-5.7960e-01, -6.5409e-02],\n",
      "        [-3.5668e-02,  3.9014e-01],\n",
      "        [ 1.3539e-01, -4.8256e-01],\n",
      "        [-2.1772e-01,  4.0386e-02],\n",
      "        [ 5.9801e-04,  3.4259e-01],\n",
      "        [ 4.4748e-01,  1.9026e-02],\n",
      "        [ 9.9598e-02, -3.2845e-01],\n",
      "        [ 1.3523e-01, -5.2961e-01],\n",
      "        [ 1.8998e-01, -2.5482e-01],\n",
      "        [-4.0916e-01, -2.3107e-01],\n",
      "        [ 2.2652e-01, -3.5434e-01],\n",
      "        [-3.3751e-02,  4.0995e-01],\n",
      "        [ 2.1124e-01, -4.7912e-01],\n",
      "        [ 1.5664e-02, -7.8370e-02],\n",
      "        [-5.4397e-02,  1.3129e-01],\n",
      "        [-1.7993e-01, -6.7514e-02],\n",
      "        [-1.7486e-01,  4.0891e-01],\n",
      "        [-1.5949e-01,  7.8922e-01],\n",
      "        [-2.4572e-02,  3.3265e-01],\n",
      "        [-4.0564e-01,  3.3927e-01],\n",
      "        [ 7.3217e-02,  1.0052e-01],\n",
      "        [-2.9690e-03,  2.7922e-01],\n",
      "        [-1.9624e-01,  2.2970e-02],\n",
      "        [-4.8999e-01, -2.7516e-02],\n",
      "        [ 1.0464e-02,  4.2397e-01],\n",
      "        [ 2.2789e-01, -3.6278e-01],\n",
      "        [ 2.0842e-01, -4.8409e-01],\n",
      "        [ 1.1272e-01, -5.1231e-01],\n",
      "        [-1.4750e-01, -1.7019e-01],\n",
      "        [-2.0052e-01,  2.9828e-01],\n",
      "        [-2.0117e-01,  1.0990e-01],\n",
      "        [-2.3045e-01,  6.1365e-02],\n",
      "        [ 2.3044e-01, -3.9501e-01],\n",
      "        [ 6.7083e-02, -5.0142e-01],\n",
      "        [-6.0219e-03, -1.3136e-01],\n",
      "        [-2.3401e-01,  8.0124e-02],\n",
      "        [-4.3068e-01,  1.9298e-01],\n",
      "        [-2.0199e-01,  2.5625e-01],\n",
      "        [-1.2712e-03, -1.3807e-01],\n",
      "        [ 4.0106e-01,  3.1332e-01],\n",
      "        [ 1.9497e-01, -2.6417e-01],\n",
      "        [-2.1586e-01,  4.7509e-01],\n",
      "        [ 1.0207e-01, -2.9350e-01],\n",
      "        [-3.9316e-01,  1.9547e-01],\n",
      "        [ 1.0652e-01, -7.6863e-02],\n",
      "        [ 6.1173e-02, -2.1523e-01],\n",
      "        [ 5.2916e-03,  4.3829e-01],\n",
      "        [-5.2751e-02, -3.7251e-02],\n",
      "        [ 9.9906e-02, -1.4257e-02],\n",
      "        [ 9.9598e-02, -3.2845e-01],\n",
      "        [ 1.4735e-01, -3.2572e-01]], dtype=torch.float64,\n",
      "       grad_fn=<SubBackward0>)\n",
      "370 \t training  0.16749031008760382\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 0.1836,  0.4959],\n",
      "        [-0.1745,  0.4011],\n",
      "        [-0.4041, -0.2287],\n",
      "        [ 0.2198, -0.4545],\n",
      "        [ 0.2029,  0.4884],\n",
      "        [-0.0421,  0.2163],\n",
      "        [-0.2070,  0.3069],\n",
      "        [ 0.1851, -0.2450],\n",
      "        [ 0.2023, -0.2944],\n",
      "        [ 0.2470, -0.3564],\n",
      "        [-0.0242, -0.0554],\n",
      "        [ 0.0373,  0.1858],\n",
      "        [-0.1388,  0.4387],\n",
      "        [-0.5686, -0.0648],\n",
      "        [-0.0384,  0.3836],\n",
      "        [ 0.1339, -0.4768],\n",
      "        [-0.2138,  0.0376],\n",
      "        [-0.0039,  0.3327],\n",
      "        [ 0.4450,  0.0199],\n",
      "        [ 0.1009, -0.3258],\n",
      "        [ 0.1359, -0.5253],\n",
      "        [ 0.1903, -0.2543],\n",
      "        [-0.4041, -0.2287],\n",
      "        [ 0.2259, -0.3530],\n",
      "        [-0.0368,  0.4029],\n",
      "        [ 0.2103, -0.4764],\n",
      "        [ 0.0185, -0.0795],\n",
      "        [-0.0576,  0.1240],\n",
      "        [-0.1833, -0.0579],\n",
      "        [-0.1723,  0.4052],\n",
      "        [-0.1686,  0.7970],\n",
      "        [-0.0267,  0.3273],\n",
      "        [-0.3962,  0.3350],\n",
      "        [ 0.0720,  0.0980],\n",
      "        [-0.0047,  0.2747],\n",
      "        [-0.1922,  0.0205],\n",
      "        [-0.4766, -0.0275],\n",
      "        [ 0.0060,  0.4142],\n",
      "        [ 0.2272, -0.3613],\n",
      "        [ 0.2075, -0.4813],\n",
      "        [ 0.1140, -0.5078],\n",
      "        [-0.1498, -0.1604],\n",
      "        [-0.2070,  0.3069],\n",
      "        [-0.2062,  0.1190],\n",
      "        [-0.2268,  0.0583],\n",
      "        [ 0.2295, -0.3933],\n",
      "        [ 0.0688, -0.4948],\n",
      "        [-0.0071, -0.1344],\n",
      "        [-0.2306,  0.0767],\n",
      "        [-0.4208,  0.1907],\n",
      "        [-0.2082,  0.2650],\n",
      "        [-0.0024, -0.1410],\n",
      "        [ 0.4012,  0.3118],\n",
      "        [ 0.1952, -0.2636],\n",
      "        [-0.2079,  0.4704],\n",
      "        [ 0.1031, -0.2914],\n",
      "        [-0.3833,  0.1932],\n",
      "        [ 0.1061, -0.0775],\n",
      "        [ 0.0598, -0.2172],\n",
      "        [ 0.0010,  0.4288],\n",
      "        [-0.0543, -0.0418],\n",
      "        [ 0.0992, -0.0156],\n",
      "        [ 0.1009, -0.3258],\n",
      "        [ 0.1451, -0.3266]], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "380 \t training  0.16430473334201182\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 0.1864,  0.4929],\n",
      "        [-0.1722,  0.3966],\n",
      "        [-0.4001, -0.2272],\n",
      "        [ 0.2183, -0.4519],\n",
      "        [ 0.2054,  0.4855],\n",
      "        [-0.0415,  0.2144],\n",
      "        [-0.2145,  0.3156],\n",
      "        [ 0.1849, -0.2444],\n",
      "        [ 0.1988, -0.2914],\n",
      "        [ 0.2426, -0.3524],\n",
      "        [-0.0213, -0.0565],\n",
      "        [ 0.0364,  0.1823],\n",
      "        [-0.1364,  0.4332],\n",
      "        [-0.5582, -0.0653],\n",
      "        [-0.0410,  0.3766],\n",
      "        [ 0.1312, -0.4711],\n",
      "        [-0.2097,  0.0354],\n",
      "        [-0.0085,  0.3226],\n",
      "        [ 0.4411,  0.0208],\n",
      "        [ 0.1031, -0.3223],\n",
      "        [ 0.1368, -0.5205],\n",
      "        [ 0.1900, -0.2536],\n",
      "        [-0.4001, -0.2272],\n",
      "        [ 0.2245, -0.3514],\n",
      "        [-0.0398,  0.3953],\n",
      "        [ 0.2089, -0.4734],\n",
      "        [ 0.0209, -0.0803],\n",
      "        [-0.0606,  0.1167],\n",
      "        [-0.1872, -0.0479],\n",
      "        [-0.1700,  0.4006],\n",
      "        [-0.1788,  0.8049],\n",
      "        [-0.0286,  0.3216],\n",
      "        [-0.3863,  0.3313],\n",
      "        [ 0.0715,  0.0956],\n",
      "        [-0.0062,  0.2699],\n",
      "        [-0.1881,  0.0185],\n",
      "        [-0.4636, -0.0286],\n",
      "        [ 0.0015,  0.4039],\n",
      "        [ 0.2258, -0.3597],\n",
      "        [ 0.2062, -0.4782],\n",
      "        [ 0.1158, -0.5027],\n",
      "        [-0.1524, -0.1501],\n",
      "        [-0.2145,  0.3156],\n",
      "        [-0.2120,  0.1284],\n",
      "        [-0.2229,  0.0556],\n",
      "        [ 0.2280, -0.3913],\n",
      "        [ 0.0710, -0.4872],\n",
      "        [-0.0083, -0.1372],\n",
      "        [-0.2269,  0.0737],\n",
      "        [-0.4098,  0.1891],\n",
      "        [-0.2154,  0.2738],\n",
      "        [-0.0036, -0.1437],\n",
      "        [ 0.4001,  0.3106],\n",
      "        [ 0.1948, -0.2628],\n",
      "        [-0.2000,  0.4662],\n",
      "        [ 0.1052, -0.2883],\n",
      "        [-0.3724,  0.1916],\n",
      "        [ 0.1067, -0.0775],\n",
      "        [ 0.0583, -0.2189],\n",
      "        [-0.0034,  0.4188],\n",
      "        [-0.0558, -0.0460],\n",
      "        [ 0.0993, -0.0165],\n",
      "        [ 0.1031, -0.3223],\n",
      "        [ 0.1427, -0.3272]], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "390 \t training  0.16102884392829742\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 0.1884,  0.4903],\n",
      "        [-0.1701,  0.3913],\n",
      "        [-0.3970, -0.2263],\n",
      "        [ 0.2163, -0.4490],\n",
      "        [ 0.2071,  0.4831],\n",
      "        [-0.0409,  0.2120],\n",
      "        [-0.2232,  0.3244],\n",
      "        [ 0.1841, -0.2433],\n",
      "        [ 0.1946, -0.2888],\n",
      "        [ 0.2368, -0.3486],\n",
      "        [-0.0186, -0.0570],\n",
      "        [ 0.0361,  0.1787],\n",
      "        [-0.1342,  0.4268],\n",
      "        [-0.5484, -0.0668],\n",
      "        [-0.0436,  0.3689],\n",
      "        [ 0.1274, -0.4656],\n",
      "        [-0.2054,  0.0336],\n",
      "        [-0.0131,  0.3120],\n",
      "        [ 0.4357,  0.0220],\n",
      "        [ 0.1063, -0.3178],\n",
      "        [ 0.1378, -0.5152],\n",
      "        [ 0.1891, -0.2525],\n",
      "        [-0.3970, -0.2263],\n",
      "        [ 0.2226, -0.3496],\n",
      "        [-0.0428,  0.3870],\n",
      "        [ 0.2071, -0.4702],\n",
      "        [ 0.0230, -0.0806],\n",
      "        [-0.0637,  0.1093],\n",
      "        [-0.1918, -0.0376],\n",
      "        [-0.1679,  0.3952],\n",
      "        [-0.1902,  0.8129],\n",
      "        [-0.0302,  0.3154],\n",
      "        [-0.3760,  0.3286],\n",
      "        [ 0.0718,  0.0934],\n",
      "        [-0.0072,  0.2648],\n",
      "        [-0.1838,  0.0171],\n",
      "        [-0.4508, -0.0307],\n",
      "        [-0.0032,  0.3930],\n",
      "        [ 0.2238, -0.3578],\n",
      "        [ 0.2045, -0.4749],\n",
      "        [ 0.1179, -0.4969],\n",
      "        [-0.1555, -0.1393],\n",
      "        [-0.2232,  0.3244],\n",
      "        [-0.2189,  0.1379],\n",
      "        [-0.2187,  0.0534],\n",
      "        [ 0.2258, -0.3891],\n",
      "        [ 0.0737, -0.4788],\n",
      "        [-0.0096, -0.1396],\n",
      "        [-0.2229,  0.0711],\n",
      "        [-0.3978,  0.1884],\n",
      "        [-0.2237,  0.2827],\n",
      "        [-0.0049, -0.1460],\n",
      "        [ 0.3978,  0.3099],\n",
      "        [ 0.1937, -0.2616],\n",
      "        [-0.1922,  0.4626],\n",
      "        [ 0.1083, -0.2843],\n",
      "        [-0.3604,  0.1907],\n",
      "        [ 0.1083, -0.0769],\n",
      "        [ 0.0566, -0.2202],\n",
      "        [-0.0079,  0.4082],\n",
      "        [-0.0574, -0.0501],\n",
      "        [ 0.1004, -0.0169],\n",
      "        [ 0.1063, -0.3178],\n",
      "        [ 0.1401, -0.3274]], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "400 \t training  0.15766329986331779\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 0.1895,  0.4884],\n",
      "        [-0.1681,  0.3851],\n",
      "        [-0.3947, -0.2258],\n",
      "        [ 0.2138, -0.4458],\n",
      "        [ 0.2078,  0.4812],\n",
      "        [-0.0404,  0.2091],\n",
      "        [-0.2330,  0.3331],\n",
      "        [ 0.1827, -0.2418],\n",
      "        [ 0.1899, -0.2864],\n",
      "        [ 0.2297, -0.3448],\n",
      "        [-0.0162, -0.0568],\n",
      "        [ 0.0363,  0.1751],\n",
      "        [-0.1320,  0.4196],\n",
      "        [-0.5391, -0.0693],\n",
      "        [-0.0461,  0.3607],\n",
      "        [ 0.1223, -0.4601],\n",
      "        [-0.2009,  0.0325],\n",
      "        [-0.0178,  0.3009],\n",
      "        [ 0.4290,  0.0236],\n",
      "        [ 0.1105, -0.3125],\n",
      "        [ 0.1390, -0.5095],\n",
      "        [ 0.1876, -0.2509],\n",
      "        [-0.3947, -0.2258],\n",
      "        [ 0.2201, -0.3473],\n",
      "        [-0.0457,  0.3781],\n",
      "        [ 0.2050, -0.4666],\n",
      "        [ 0.0248, -0.0802],\n",
      "        [-0.0668,  0.1019],\n",
      "        [-0.1973, -0.0272],\n",
      "        [-0.1659,  0.3889],\n",
      "        [-0.2026,  0.8210],\n",
      "        [-0.0316,  0.3088],\n",
      "        [-0.3650,  0.3268],\n",
      "        [ 0.0728,  0.0914],\n",
      "        [-0.0079,  0.2594],\n",
      "        [-0.1794,  0.0163],\n",
      "        [-0.4382, -0.0340],\n",
      "        [-0.0079,  0.3815],\n",
      "        [ 0.2213, -0.3555],\n",
      "        [ 0.2024, -0.4713],\n",
      "        [ 0.1203, -0.4905],\n",
      "        [-0.1592, -0.1282],\n",
      "        [-0.2330,  0.3331],\n",
      "        [-0.2268,  0.1473],\n",
      "        [-0.2142,  0.0518],\n",
      "        [ 0.2232, -0.3866],\n",
      "        [ 0.0765, -0.4697],\n",
      "        [-0.0110, -0.1416],\n",
      "        [-0.2185,  0.0691],\n",
      "        [-0.3847,  0.1885],\n",
      "        [-0.2331,  0.2916],\n",
      "        [-0.0062, -0.1479],\n",
      "        [ 0.3943,  0.3096],\n",
      "        [ 0.1921, -0.2600],\n",
      "        [-0.1846,  0.4598],\n",
      "        [ 0.1123, -0.2795],\n",
      "        [-0.3473,  0.1905],\n",
      "        [ 0.1108, -0.0757],\n",
      "        [ 0.0548, -0.2211],\n",
      "        [-0.0125,  0.3968],\n",
      "        [-0.0589, -0.0538],\n",
      "        [ 0.1024, -0.0168],\n",
      "        [ 0.1105, -0.3125],\n",
      "        [ 0.1373, -0.3272]], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "410 \t training  0.1542171722506505\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 0.1898,  0.4871],\n",
      "        [-0.1660,  0.3781],\n",
      "        [-0.3928, -0.2256],\n",
      "        [ 0.2110, -0.4422],\n",
      "        [ 0.2077,  0.4801],\n",
      "        [-0.0397,  0.2058],\n",
      "        [-0.2440,  0.3416],\n",
      "        [ 0.1809, -0.2397],\n",
      "        [ 0.1849, -0.2837],\n",
      "        [ 0.2212, -0.3409],\n",
      "        [-0.0140, -0.0558],\n",
      "        [ 0.0372,  0.1714],\n",
      "        [-0.1298,  0.4115],\n",
      "        [-0.5300, -0.0726],\n",
      "        [-0.0483,  0.3519],\n",
      "        [ 0.1161, -0.4547],\n",
      "        [-0.1962,  0.0321],\n",
      "        [-0.0227,  0.2893],\n",
      "        [ 0.4210,  0.0256],\n",
      "        [ 0.1154, -0.3065],\n",
      "        [ 0.1402, -0.5034],\n",
      "        [ 0.1856, -0.2487],\n",
      "        [-0.3928, -0.2256],\n",
      "        [ 0.2171, -0.3446],\n",
      "        [-0.0485,  0.3686],\n",
      "        [ 0.2025, -0.4627],\n",
      "        [ 0.0263, -0.0790],\n",
      "        [-0.0700,  0.0945],\n",
      "        [-0.2039, -0.0169],\n",
      "        [-0.1638,  0.3818],\n",
      "        [-0.2159,  0.8290],\n",
      "        [-0.0326,  0.3017],\n",
      "        [-0.3536,  0.3261],\n",
      "        [ 0.0746,  0.0896],\n",
      "        [-0.0081,  0.2537],\n",
      "        [-0.1748,  0.0163],\n",
      "        [-0.4257, -0.0383],\n",
      "        [-0.0129,  0.3693],\n",
      "        [ 0.2182, -0.3527],\n",
      "        [ 0.2001, -0.4673],\n",
      "        [ 0.1230, -0.4837],\n",
      "        [-0.1638, -0.1172],\n",
      "        [-0.2440,  0.3416],\n",
      "        [-0.2359,  0.1566],\n",
      "        [-0.2096,  0.0509],\n",
      "        [ 0.2200, -0.3836],\n",
      "        [ 0.0794, -0.4601],\n",
      "        [-0.0124, -0.1431],\n",
      "        [-0.2140,  0.0676],\n",
      "        [-0.3707,  0.1895],\n",
      "        [-0.2437,  0.3003],\n",
      "        [-0.0077, -0.1493],\n",
      "        [ 0.3898,  0.3099],\n",
      "        [ 0.1900, -0.2577],\n",
      "        [-0.1770,  0.4578],\n",
      "        [ 0.1171, -0.2740],\n",
      "        [-0.3334,  0.1910],\n",
      "        [ 0.1143, -0.0740],\n",
      "        [ 0.0528, -0.2214],\n",
      "        [-0.0172,  0.3848],\n",
      "        [-0.0606, -0.0572],\n",
      "        [ 0.1053, -0.0163],\n",
      "        [ 0.1154, -0.3065],\n",
      "        [ 0.1344, -0.3264]], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "420 \t training  0.15070674172331353\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 0.1892,  0.4865],\n",
      "        [-0.1638,  0.3704],\n",
      "        [-0.3911, -0.2255],\n",
      "        [ 0.2078, -0.4382],\n",
      "        [ 0.2068,  0.4795],\n",
      "        [-0.0389,  0.2021],\n",
      "        [-0.2559,  0.3500],\n",
      "        [ 0.1786, -0.2368],\n",
      "        [ 0.1799, -0.2808],\n",
      "        [ 0.2115, -0.3370],\n",
      "        [-0.0120, -0.0539],\n",
      "        [ 0.0386,  0.1677],\n",
      "        [-0.1275,  0.4027],\n",
      "        [-0.5211, -0.0767],\n",
      "        [-0.0503,  0.3426],\n",
      "        [ 0.1089, -0.4494],\n",
      "        [-0.1915,  0.0325],\n",
      "        [-0.0278,  0.2772],\n",
      "        [ 0.4119,  0.0281],\n",
      "        [ 0.1209, -0.3001],\n",
      "        [ 0.1414, -0.4968],\n",
      "        [ 0.1832, -0.2457],\n",
      "        [-0.3911, -0.2255],\n",
      "        [ 0.2137, -0.3412],\n",
      "        [-0.0510,  0.3585],\n",
      "        [ 0.1997, -0.4584],\n",
      "        [ 0.0275, -0.0770],\n",
      "        [-0.0734,  0.0871],\n",
      "        [-0.2113, -0.0068],\n",
      "        [-0.1616,  0.3740],\n",
      "        [-0.2299,  0.8371],\n",
      "        [-0.0333,  0.2942],\n",
      "        [-0.3418,  0.3266],\n",
      "        [ 0.0770,  0.0879],\n",
      "        [-0.0078,  0.2476],\n",
      "        [-0.1703,  0.0172],\n",
      "        [-0.4131, -0.0435],\n",
      "        [-0.0178,  0.3563],\n",
      "        [ 0.2148, -0.3493],\n",
      "        [ 0.1974, -0.4629],\n",
      "        [ 0.1260, -0.4765],\n",
      "        [-0.1693, -0.1063],\n",
      "        [-0.2559,  0.3500],\n",
      "        [-0.2459,  0.1656],\n",
      "        [-0.2048,  0.0506],\n",
      "        [ 0.2165, -0.3800],\n",
      "        [ 0.0823, -0.4502],\n",
      "        [-0.0140, -0.1439],\n",
      "        [-0.2095,  0.0667],\n",
      "        [-0.3560,  0.1913],\n",
      "        [-0.2552,  0.3087],\n",
      "        [-0.0093, -0.1500],\n",
      "        [ 0.3842,  0.3108],\n",
      "        [ 0.1875, -0.2547],\n",
      "        [-0.1697,  0.4568],\n",
      "        [ 0.1227, -0.2680],\n",
      "        [-0.3188,  0.1921],\n",
      "        [ 0.1185, -0.0719],\n",
      "        [ 0.0507, -0.2210],\n",
      "        [-0.0219,  0.3720],\n",
      "        [-0.0623, -0.0601],\n",
      "        [ 0.1089, -0.0155],\n",
      "        [ 0.1209, -0.3001],\n",
      "        [ 0.1315, -0.3250]], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "430 \t training  0.14715392677804778\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 0.1876,  0.4865],\n",
      "        [-0.1614,  0.3620],\n",
      "        [-0.3894, -0.2252],\n",
      "        [ 0.2044, -0.4336],\n",
      "        [ 0.2048,  0.4796],\n",
      "        [-0.0379,  0.1981],\n",
      "        [-0.2684,  0.3581],\n",
      "        [ 0.1759, -0.2331],\n",
      "        [ 0.1749, -0.2772],\n",
      "        [ 0.2009, -0.3329],\n",
      "        [-0.0102, -0.0510],\n",
      "        [ 0.0406,  0.1637],\n",
      "        [-0.1248,  0.3932],\n",
      "        [-0.5122, -0.0815],\n",
      "        [-0.0519,  0.3328],\n",
      "        [ 0.1008, -0.4440],\n",
      "        [-0.1867,  0.0337],\n",
      "        [-0.0331,  0.2645],\n",
      "        [ 0.4017,  0.0311],\n",
      "        [ 0.1270, -0.2933],\n",
      "        [ 0.1428, -0.4900],\n",
      "        [ 0.1804, -0.2420],\n",
      "        [-0.3894, -0.2252],\n",
      "        [ 0.2099, -0.3371],\n",
      "        [-0.0532,  0.3479],\n",
      "        [ 0.1967, -0.4536],\n",
      "        [ 0.0284, -0.0740],\n",
      "        [-0.0770,  0.0796],\n",
      "        [-0.2196,  0.0029],\n",
      "        [-0.1591,  0.3655],\n",
      "        [-0.2443,  0.8451],\n",
      "        [-0.0335,  0.2863],\n",
      "        [-0.3299,  0.3283],\n",
      "        [ 0.0800,  0.0861],\n",
      "        [-0.0071,  0.2413],\n",
      "        [-0.1657,  0.0190],\n",
      "        [-0.4003, -0.0497],\n",
      "        [-0.0228,  0.3428],\n",
      "        [ 0.2110, -0.3452],\n",
      "        [ 0.1946, -0.4580],\n",
      "        [ 0.1291, -0.4690],\n",
      "        [-0.1755, -0.0958],\n",
      "        [-0.2684,  0.3581],\n",
      "        [-0.2567,  0.1743],\n",
      "        [-0.2001,  0.0511],\n",
      "        [ 0.2126, -0.3758],\n",
      "        [ 0.0850, -0.4403],\n",
      "        [-0.0157, -0.1441],\n",
      "        [-0.2049,  0.0665],\n",
      "        [-0.3408,  0.1938],\n",
      "        [-0.2675,  0.3169],\n",
      "        [-0.0110, -0.1500],\n",
      "        [ 0.3775,  0.3122],\n",
      "        [ 0.1845, -0.2509],\n",
      "        [-0.1627,  0.4567],\n",
      "        [ 0.1288, -0.2617],\n",
      "        [-0.3039,  0.1936],\n",
      "        [ 0.1233, -0.0697],\n",
      "        [ 0.0485, -0.2198],\n",
      "        [-0.0266,  0.3587],\n",
      "        [-0.0643, -0.0625],\n",
      "        [ 0.1131, -0.0146],\n",
      "        [ 0.1270, -0.2933],\n",
      "        [ 0.1285, -0.3228]], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "440 \t training  0.14358507185261318\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 0.1851,  0.4869],\n",
      "        [-0.1585,  0.3532],\n",
      "        [-0.3874, -0.2247],\n",
      "        [ 0.2008, -0.4284],\n",
      "        [ 0.2018,  0.4800],\n",
      "        [-0.0367,  0.1939],\n",
      "        [-0.2813,  0.3659],\n",
      "        [ 0.1728, -0.2286],\n",
      "        [ 0.1702, -0.2731],\n",
      "        [ 0.1895, -0.3286],\n",
      "        [-0.0088, -0.0473],\n",
      "        [ 0.0429,  0.1596],\n",
      "        [-0.1217,  0.3832],\n",
      "        [-0.5030, -0.0866],\n",
      "        [-0.0531,  0.3225],\n",
      "        [ 0.0921, -0.4387],\n",
      "        [-0.1821,  0.0358],\n",
      "        [-0.0384,  0.2515],\n",
      "        [ 0.3906,  0.0345],\n",
      "        [ 0.1336, -0.2864],\n",
      "        [ 0.1443, -0.4829],\n",
      "        [ 0.1772, -0.2375],\n",
      "        [-0.3874, -0.2247],\n",
      "        [ 0.2059, -0.3324],\n",
      "        [-0.0551,  0.3369],\n",
      "        [ 0.1936, -0.4482],\n",
      "        [ 0.0290, -0.0701],\n",
      "        [-0.0808,  0.0723],\n",
      "        [-0.2284,  0.0122],\n",
      "        [-0.1563,  0.3565],\n",
      "        [-0.2588,  0.8529],\n",
      "        [-0.0334,  0.2780],\n",
      "        [-0.3180,  0.3310],\n",
      "        [ 0.0834,  0.0842],\n",
      "        [-0.0061,  0.2347],\n",
      "        [-0.1613,  0.0217],\n",
      "        [-0.3873, -0.0565],\n",
      "        [-0.0275,  0.3289],\n",
      "        [ 0.2069, -0.3404],\n",
      "        [ 0.1916, -0.4526],\n",
      "        [ 0.1325, -0.4613],\n",
      "        [-0.1824, -0.0857],\n",
      "        [-0.2813,  0.3659],\n",
      "        [-0.2679,  0.1826],\n",
      "        [-0.1956,  0.0523],\n",
      "        [ 0.2085, -0.3709],\n",
      "        [ 0.0875, -0.4305],\n",
      "        [-0.0175, -0.1435],\n",
      "        [-0.2005,  0.0668],\n",
      "        [-0.3256,  0.1968],\n",
      "        [-0.2800,  0.3248],\n",
      "        [-0.0128, -0.1493],\n",
      "        [ 0.3698,  0.3140],\n",
      "        [ 0.1812, -0.2464],\n",
      "        [-0.1563,  0.4573],\n",
      "        [ 0.1353, -0.2553],\n",
      "        [-0.2889,  0.1953],\n",
      "        [ 0.1286, -0.0674],\n",
      "        [ 0.0463, -0.2178],\n",
      "        [-0.0309,  0.3450],\n",
      "        [-0.0663, -0.0644],\n",
      "        [ 0.1178, -0.0138],\n",
      "        [ 0.1336, -0.2864],\n",
      "        [ 0.1255, -0.3199]], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "450 \t training  0.1400301383111432\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 0.1814,  0.4874],\n",
      "        [-0.1553,  0.3441],\n",
      "        [-0.3852, -0.2236],\n",
      "        [ 0.1972, -0.4228],\n",
      "        [ 0.1977,  0.4806],\n",
      "        [-0.0354,  0.1895],\n",
      "        [-0.2942,  0.3733],\n",
      "        [ 0.1695, -0.2234],\n",
      "        [ 0.1657, -0.2683],\n",
      "        [ 0.1777, -0.3241],\n",
      "        [-0.0078, -0.0427],\n",
      "        [ 0.0455,  0.1552],\n",
      "        [-0.1181,  0.3730],\n",
      "        [-0.4935, -0.0919],\n",
      "        [-0.0540,  0.3120],\n",
      "        [ 0.0830, -0.4332],\n",
      "        [-0.1778,  0.0385],\n",
      "        [-0.0436,  0.2384],\n",
      "        [ 0.3787,  0.0380],\n",
      "        [ 0.1404, -0.2795],\n",
      "        [ 0.1459, -0.4756],\n",
      "        [ 0.1737, -0.2323],\n",
      "        [-0.3852, -0.2236],\n",
      "        [ 0.2016, -0.3270],\n",
      "        [-0.0566,  0.3256],\n",
      "        [ 0.1905, -0.4424],\n",
      "        [ 0.0292, -0.0655],\n",
      "        [-0.0847,  0.0653],\n",
      "        [-0.2375,  0.0210],\n",
      "        [-0.1530,  0.3473],\n",
      "        [-0.2730,  0.8606],\n",
      "        [-0.0330,  0.2695],\n",
      "        [-0.3064,  0.3345],\n",
      "        [ 0.0870,  0.0821],\n",
      "        [-0.0048,  0.2278],\n",
      "        [-0.1572,  0.0250],\n",
      "        [-0.3739, -0.0638],\n",
      "        [-0.0321,  0.3148],\n",
      "        [ 0.2026, -0.3350],\n",
      "        [ 0.1887, -0.4467],\n",
      "        [ 0.1361, -0.4534],\n",
      "        [-0.1896, -0.0761],\n",
      "        [-0.2942,  0.3733],\n",
      "        [-0.2793,  0.1905],\n",
      "        [-0.1912,  0.0540],\n",
      "        [ 0.2042, -0.3654],\n",
      "        [ 0.0900, -0.4209],\n",
      "        [-0.0193, -0.1421],\n",
      "        [-0.1963,  0.0677],\n",
      "        [-0.3105,  0.2001],\n",
      "        [-0.2926,  0.3323],\n",
      "        [-0.0147, -0.1478],\n",
      "        [ 0.3610,  0.3160],\n",
      "        [ 0.1776, -0.2412],\n",
      "        [-0.1507,  0.4584],\n",
      "        [ 0.1421, -0.2490],\n",
      "        [-0.2741,  0.1972],\n",
      "        [ 0.1341, -0.0653],\n",
      "        [ 0.0440, -0.2150],\n",
      "        [-0.0350,  0.3310],\n",
      "        [-0.0684, -0.0657],\n",
      "        [ 0.1227, -0.0131],\n",
      "        [ 0.1404, -0.2795],\n",
      "        [ 0.1227, -0.3162]], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "460 \t training  0.13652158385047783\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 0.1766,  0.4878],\n",
      "        [-0.1518,  0.3349],\n",
      "        [-0.3826, -0.2219],\n",
      "        [ 0.1936, -0.4166],\n",
      "        [ 0.1925,  0.4810],\n",
      "        [-0.0341,  0.1850],\n",
      "        [-0.3067,  0.3804],\n",
      "        [ 0.1659, -0.2178],\n",
      "        [ 0.1614, -0.2628],\n",
      "        [ 0.1657, -0.3195],\n",
      "        [-0.0071, -0.0377],\n",
      "        [ 0.0481,  0.1505],\n",
      "        [-0.1142,  0.3627],\n",
      "        [-0.4838, -0.0971],\n",
      "        [-0.0546,  0.3014],\n",
      "        [ 0.0739, -0.4278],\n",
      "        [-0.1738,  0.0416],\n",
      "        [-0.0486,  0.2254],\n",
      "        [ 0.3661,  0.0416],\n",
      "        [ 0.1474, -0.2728],\n",
      "        [ 0.1479, -0.4680],\n",
      "        [ 0.1700, -0.2267],\n",
      "        [-0.3826, -0.2219],\n",
      "        [ 0.1973, -0.3212],\n",
      "        [-0.0577,  0.3142],\n",
      "        [ 0.1876, -0.4362],\n",
      "        [ 0.0290, -0.0603],\n",
      "        [-0.0886,  0.0586],\n",
      "        [-0.2466,  0.0292],\n",
      "        [-0.1494,  0.3380],\n",
      "        [-0.2866,  0.8679],\n",
      "        [-0.0325,  0.2608],\n",
      "        [-0.2955,  0.3383],\n",
      "        [ 0.0908,  0.0797],\n",
      "        [-0.0033,  0.2206],\n",
      "        [-0.1535,  0.0289],\n",
      "        [-0.3603, -0.0712],\n",
      "        [-0.0362,  0.3007],\n",
      "        [ 0.1983, -0.3291],\n",
      "        [ 0.1859, -0.4404],\n",
      "        [ 0.1399, -0.4453],\n",
      "        [-0.1970, -0.0672],\n",
      "        [-0.3067,  0.3804],\n",
      "        [-0.2904,  0.1980],\n",
      "        [-0.1872,  0.0562],\n",
      "        [ 0.2000, -0.3595],\n",
      "        [ 0.0923, -0.4117],\n",
      "        [-0.0212, -0.1401],\n",
      "        [-0.1925,  0.0690],\n",
      "        [-0.2960,  0.2033],\n",
      "        [-0.3049,  0.3395],\n",
      "        [-0.0165, -0.1456],\n",
      "        [ 0.3513,  0.3177],\n",
      "        [ 0.1738, -0.2356],\n",
      "        [-0.1460,  0.4595],\n",
      "        [ 0.1492, -0.2429],\n",
      "        [-0.2599,  0.1989],\n",
      "        [ 0.1398, -0.0634],\n",
      "        [ 0.0418, -0.2115],\n",
      "        [-0.0387,  0.3170],\n",
      "        [-0.0705, -0.0663],\n",
      "        [ 0.1278, -0.0127],\n",
      "        [ 0.1474, -0.2728],\n",
      "        [ 0.1201, -0.3118]], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "470 \t training  0.1330920410411721\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 0.1707,  0.4875],\n",
      "        [-0.1480,  0.3259],\n",
      "        [-0.3795, -0.2194],\n",
      "        [ 0.1904, -0.4102],\n",
      "        [ 0.1862,  0.4808],\n",
      "        [-0.0329,  0.1806],\n",
      "        [-0.3185,  0.3871],\n",
      "        [ 0.1622, -0.2120],\n",
      "        [ 0.1575, -0.2566],\n",
      "        [ 0.1537, -0.3149],\n",
      "        [-0.0068, -0.0325],\n",
      "        [ 0.0507,  0.1456],\n",
      "        [-0.1099,  0.3525],\n",
      "        [-0.4737, -0.1019],\n",
      "        [-0.0551,  0.2908],\n",
      "        [ 0.0650, -0.4223],\n",
      "        [-0.1702,  0.0451],\n",
      "        [-0.0534,  0.2127],\n",
      "        [ 0.3531,  0.0447],\n",
      "        [ 0.1545, -0.2664],\n",
      "        [ 0.1501, -0.4603],\n",
      "        [ 0.1662, -0.2208],\n",
      "        [-0.3795, -0.2194],\n",
      "        [ 0.1930, -0.3150],\n",
      "        [-0.0587,  0.3029],\n",
      "        [ 0.1849, -0.4296],\n",
      "        [ 0.0285, -0.0551],\n",
      "        [-0.0923,  0.0525],\n",
      "        [-0.2554,  0.0369],\n",
      "        [-0.1456,  0.3289],\n",
      "        [-0.2994,  0.8748],\n",
      "        [-0.0319,  0.2520],\n",
      "        [-0.2856,  0.3420],\n",
      "        [ 0.0944,  0.0771],\n",
      "        [-0.0019,  0.2134],\n",
      "        [-0.1501,  0.0331],\n",
      "        [-0.3465, -0.0784],\n",
      "        [-0.0401,  0.2869],\n",
      "        [ 0.1940, -0.3230],\n",
      "        [ 0.1834, -0.4338],\n",
      "        [ 0.1441, -0.4372],\n",
      "        [-0.2042, -0.0589],\n",
      "        [-0.3185,  0.3871],\n",
      "        [-0.3010,  0.2050],\n",
      "        [-0.1836,  0.0586],\n",
      "        [ 0.1959, -0.3533],\n",
      "        [ 0.0947, -0.4030],\n",
      "        [-0.0228, -0.1375],\n",
      "        [-0.1890,  0.0705],\n",
      "        [-0.2823,  0.2063],\n",
      "        [-0.3165,  0.3462],\n",
      "        [-0.0182, -0.1428],\n",
      "        [ 0.3408,  0.3188],\n",
      "        [ 0.1700, -0.2297],\n",
      "        [-0.1424,  0.4604],\n",
      "        [ 0.1562, -0.2370],\n",
      "        [-0.2464,  0.2003],\n",
      "        [ 0.1454, -0.0619],\n",
      "        [ 0.0399, -0.2075],\n",
      "        [-0.0420,  0.3033],\n",
      "        [-0.0725, -0.0663],\n",
      "        [ 0.1327, -0.0126],\n",
      "        [ 0.1545, -0.2664],\n",
      "        [ 0.1179, -0.3068]], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "480 \t training  0.1297706671957985\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 1.6385e-01,  4.8621e-01],\n",
      "        [-1.4416e-01,  3.1717e-01],\n",
      "        [-3.7585e-01, -2.1603e-01],\n",
      "        [ 1.8744e-01, -4.0354e-01],\n",
      "        [ 1.7898e-01,  4.7956e-01],\n",
      "        [-3.1739e-02,  1.7623e-01],\n",
      "        [-3.2942e-01,  3.9318e-01],\n",
      "        [ 1.5855e-01, -2.0627e-01],\n",
      "        [ 1.5380e-01, -2.4971e-01],\n",
      "        [ 1.4206e-01, -3.1056e-01],\n",
      "        [-6.7701e-03, -2.7448e-02],\n",
      "        [ 5.3009e-02,  1.4061e-01],\n",
      "        [-1.0546e-01,  3.4274e-01],\n",
      "        [-4.6344e-01, -1.0603e-01],\n",
      "        [-5.5615e-02,  2.8045e-01],\n",
      "        [ 5.6623e-02, -4.1685e-01],\n",
      "        [-1.6689e-01,  4.8533e-02],\n",
      "        [-5.7859e-02,  2.0072e-01],\n",
      "        [ 3.3985e-01,  4.7115e-02],\n",
      "        [ 1.6156e-01, -2.6024e-01],\n",
      "        [ 1.5276e-01, -4.5238e-01],\n",
      "        [ 1.6250e-01, -2.1508e-01],\n",
      "        [-3.7585e-01, -2.1603e-01],\n",
      "        [ 1.8901e-01, -3.0887e-01],\n",
      "        [-5.9597e-02,  2.9189e-01],\n",
      "        [ 1.8261e-01, -4.2279e-01],\n",
      "        [ 2.7682e-02, -4.9931e-02],\n",
      "        [-9.5697e-02,  4.7042e-02],\n",
      "        [-2.6359e-01,  4.4049e-02],\n",
      "        [-1.4166e-01,  3.2008e-01],\n",
      "        [-3.1115e-01,  8.8099e-01],\n",
      "        [-3.1403e-02,  2.4329e-01],\n",
      "        [-2.7668e-01,  3.4528e-01],\n",
      "        [ 9.7761e-02,  7.4223e-02],\n",
      "        [-7.1568e-04,  2.0608e-01],\n",
      "        [-1.4706e-01,  3.7206e-02],\n",
      "        [-3.3267e-01, -8.5174e-02],\n",
      "        [-4.3612e-02,  2.7360e-01],\n",
      "        [ 1.9002e-01, -3.1680e-01],\n",
      "        [ 1.8124e-01, -4.2698e-01],\n",
      "        [ 1.4856e-01, -4.2913e-01],\n",
      "        [-2.1110e-01, -5.1221e-02],\n",
      "        [-3.2942e-01,  3.9318e-01],\n",
      "        [-3.1086e-01,  2.1149e-01],\n",
      "        [-1.8026e-01,  6.1176e-02],\n",
      "        [ 1.9206e-01, -3.4695e-01],\n",
      "        [ 9.7026e-02, -3.9470e-01],\n",
      "        [-2.4180e-02, -1.3435e-01],\n",
      "        [-1.8574e-01,  7.2091e-02],\n",
      "        [-2.6958e-01,  2.0882e-01],\n",
      "        [-3.2722e-01,  3.5233e-01],\n",
      "        [-1.9573e-02, -1.3956e-01],\n",
      "        [ 3.2958e-01,  3.1897e-01],\n",
      "        [ 1.6618e-01, -2.2388e-01],\n",
      "        [-1.3998e-01,  4.6062e-01],\n",
      "        [ 1.6320e-01, -2.3144e-01],\n",
      "        [-2.3390e-01,  2.0128e-01],\n",
      "        [ 1.5073e-01, -6.0737e-02],\n",
      "        [ 3.8275e-02, -2.0292e-01],\n",
      "        [-4.5064e-02,  2.9013e-01],\n",
      "        [-7.4196e-02, -6.5811e-02],\n",
      "        [ 1.3737e-01, -1.2801e-02],\n",
      "        [ 1.6156e-01, -2.6024e-01],\n",
      "        [ 1.1627e-01, -3.0130e-01]], dtype=torch.float64,\n",
      "       grad_fn=<SubBackward0>)\n",
      "490 \t training  0.12657930857480318\n",
      " \t validation  nan\n",
      "_________\n",
      "tensor([[ 1.5617e-01,  4.8371e-01],\n",
      "        [-1.4038e-01,  3.0901e-01],\n",
      "        [-3.7171e-01, -2.1169e-01],\n",
      "        [ 1.8497e-01, -3.9691e-01],\n",
      "        [ 1.7094e-01,  4.7710e-01],\n",
      "        [-3.0782e-02,  1.7218e-01],\n",
      "        [-3.3931e-01,  3.9868e-01],\n",
      "        [ 1.5504e-01, -2.0088e-01],\n",
      "        [ 1.5049e-01, -2.4227e-01],\n",
      "        [ 1.3099e-01, -3.0654e-01],\n",
      "        [-6.9665e-03, -2.2773e-02],\n",
      "        [ 5.4948e-02,  1.3556e-01],\n",
      "        [-1.0109e-01,  3.3349e-01],\n",
      "        [-4.5296e-01, -1.0927e-01],\n",
      "        [-5.6208e-02,  2.7045e-01],\n",
      "        [ 4.8862e-02, -4.1162e-01],\n",
      "        [-1.6387e-01,  5.1916e-02],\n",
      "        [-6.2052e-02,  1.8948e-01],\n",
      "        [ 3.2664e-01,  4.8540e-02],\n",
      "        [ 1.6839e-01, -2.5447e-01],\n",
      "        [ 1.5571e-01, -4.4449e-01],\n",
      "        [ 1.5893e-01, -2.0964e-01],\n",
      "        [-3.7171e-01, -2.1169e-01],\n",
      "        [ 1.8530e-01, -3.0289e-01],\n",
      "        [-6.0553e-02,  2.8129e-01],\n",
      "        [ 1.8078e-01, -4.1596e-01],\n",
      "        [ 2.6738e-02, -4.5215e-02],\n",
      "        [-9.8723e-02,  4.2342e-02],\n",
      "        [-2.7109e-01,  5.0551e-02],\n",
      "        [-1.3781e-01,  3.1181e-01],\n",
      "        [-3.2181e-01,  8.8648e-01],\n",
      "        [-3.1158e-02,  2.3482e-01],\n",
      "        [-2.6893e-01,  3.4772e-01],\n",
      "        [ 1.0070e-01,  7.1181e-02],\n",
      "        [ 1.9271e-04,  1.9893e-01],\n",
      "        [-1.4431e-01,  4.1179e-02],\n",
      "        [-3.1896e-01, -9.1220e-02],\n",
      "        [-4.6964e-02,  2.6100e-01],\n",
      "        [ 1.8634e-01, -3.1077e-01],\n",
      "        [ 1.7958e-01, -4.2010e-01],\n",
      "        [ 1.5322e-01, -4.2109e-01],\n",
      "        [-2.1746e-01, -4.4231e-02],\n",
      "        [-3.3931e-01,  3.9868e-01],\n",
      "        [-3.1978e-01,  2.1738e-01],\n",
      "        [-1.7718e-01,  6.3696e-02],\n",
      "        [ 1.8861e-01, -3.4074e-01],\n",
      "        [ 9.9406e-02, -3.8695e-01],\n",
      "        [-2.5181e-02, -1.3087e-01],\n",
      "        [-1.8274e-01,  7.3777e-02],\n",
      "        [-2.5786e-01,  2.1088e-01],\n",
      "        [-3.3693e-01,  3.5790e-01],\n",
      "        [-2.0574e-02, -1.3595e-01],\n",
      "        [ 3.1799e-01,  3.1789e-01],\n",
      "        [ 1.6255e-01, -2.1840e-01],\n",
      "        [-1.3862e-01,  4.5983e-01],\n",
      "        [ 1.6991e-01, -2.2630e-01],\n",
      "        [-2.2236e-01,  2.0189e-01],\n",
      "        [ 1.5565e-01, -5.9920e-02],\n",
      "        [ 3.7123e-02, -1.9809e-01],\n",
      "        [-4.7956e-02,  2.7756e-01],\n",
      "        [-7.5543e-02, -6.4775e-02],\n",
      "        [ 1.4160e-01, -1.3324e-02],\n",
      "        [ 1.6839e-01, -2.5447e-01],\n",
      "        [ 1.1517e-01, -2.9551e-01]], dtype=torch.float64,\n",
      "       grad_fn=<SubBackward0>)\n",
      "500 \t training  0.12353025942532596\n",
      " \t validation  nan\n",
      "_________\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "epoch = 500\n",
    "delta_t = 0.01\n",
    "\n",
    "func = PendelumNetwork()\n",
    "model = NeuralODE(func, delta_t)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "for k in range(epoch):\n",
    "    avg_los = []\n",
    "    for train_batch in train:\n",
    "        x_i, x_f = train_batch\n",
    "        prediction = model(x_i, propagation_time, use_adjoint = False)\n",
    "       \n",
    "        loss = torch.mean(torch.norm((prediction - x_f),dim=1)**2)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_los.append(loss.item())\n",
    "    if (k+1)%10==0:\n",
    "        avg_los_2 = []\n",
    "        for test_batch in test:\n",
    "            x_i, x_f = test_batch\n",
    "            prediction = model(x_i, propagation_time, use_adjoint=False)\n",
    "            print(torch.norm((prediction - x_f),dim=1)**2)\n",
    "            break\n",
    "            loss = torch.mean(torch.norm((prediction - x_f),dim=1)**2)\n",
    "            avg_los_2.append(loss.item())\n",
    "\n",
    "        print(k+1 , \"\\t training \",np.mean(avg_los))\n",
    "        print(\" \\t validation \",np.mean(avg_los_2))\n",
    "        print(\"_________\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction_theta = model(torch.tensor([[theta[0], omega[0]]],dtype=float), 10, True).T[0].detach().numpy()\n",
    "prediction_omega = model(torch.tensor([[theta[0], omega[0]]],dtype=float), 10, True).T[1].detach().numpy()\n",
    "\n",
    "plt.plot(np.arange(0, 10, delta_t),prediction_theta, label=\"prediction_theta\")\n",
    "plt.plot(np.arange(0, 10, delta_t),prediction_omega, label=\"prediction_omega\")\n",
    "plt.plot(t, theta,label=\"real_theta\")\n",
    "plt.plot(t, omega,label=\"real_theta\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "64453cf929eccabd5035c1f376aa137e0dc1778449c6d25ac42817d1b82d4cc0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
